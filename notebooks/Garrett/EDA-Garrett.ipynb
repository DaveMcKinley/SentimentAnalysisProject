{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a model that can rate the sentiment of a Tweet based on its content.\n",
    "\n",
    "You'll build an NLP model to analyze Twitter sentiment about Apple and Google products. The dataset comes from CrowdFlower via data.world. Human raters rated the sentiment in over 9,000 Tweets as positive, negative, or neither.\n",
    "\n",
    "Aim for a Proof of Concept There are many approaches to NLP problems - start with something simple and iterate from there. For example, you could start by limiting your analysis to positive and negative Tweets only, allowing you to build a binary classifier. Then you could add in the neutral Tweets to build out a multiclass classifier. You may also consider using some of the more advanced NLP methods in the Mod 4 Appendix.\n",
    "\n",
    "Evaluation Evaluating multiclass classifiers can be trickier than binary classifiers because there are multiple ways to mis-classify an observation, and some errors are more problematic than others. Use the business problem that your NLP project sets out to solve to inform your choice of evaluation metrics.\n",
    "\n",
    "Data: https://data.world/crowdflower/brands-and-product-emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: graphviz in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from catboost) (0.17)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from catboost) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from catboost) (1.21.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from catboost) (1.6.2)\n",
      "Requirement already satisfied: six in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from catboost) (3.4.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from catboost) (5.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pandas>=0.24.0->catboost) (2021.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from matplotlib->catboost) (8.3.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages (from plotly->catboost) (6.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk related imports\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "judge-1377884607_tweet_product_company.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   \n",
       "9090  Google's Zeiger, a physician never reported po...   \n",
       "9091  Some Verizon iPhone customers complained their...   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "9088                            iPad   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                      Negative emotion  \n",
       "1                                      Positive emotion  \n",
       "2                                      Positive emotion  \n",
       "3                                      Negative emotion  \n",
       "4                                      Positive emotion  \n",
       "...                                                 ...  \n",
       "9088                                   Positive emotion  \n",
       "9089                 No emotion toward brand or product  \n",
       "9090                 No emotion toward brand or product  \n",
       "9091                 No emotion toward brand or product  \n",
       "9092                 No emotion toward brand or product  \n",
       "\n",
       "[9093 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/judge-1377884607_tweet_product_company.csv', encoding = 'unicode_escape')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_tweet = df['tweet_text'][0]\n",
    "first_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.@wesley83 i have a 3g iphone. after 3 hrs tweeting at #rise_austin, it was dead!  i need to upgrade. plugin stations at #sxsw.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lowercase\n",
    "first_tweet_lower = first_tweet.lower()\n",
    "first_tweet_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'i',\n",
       " 'have',\n",
       " 'a',\n",
       " '3g',\n",
       " 'iphone',\n",
       " '.',\n",
       " 'after',\n",
       " '3',\n",
       " 'hrs',\n",
       " 'tweeting',\n",
       " 'at',\n",
       " '#rise_austin',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'dead',\n",
       " '!',\n",
       " 'i',\n",
       " 'need',\n",
       " 'to',\n",
       " 'upgrade',\n",
       " '.',\n",
       " 'plugin',\n",
       " 'stations',\n",
       " 'at',\n",
       " '#sxsw',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweet tokenizer\n",
    "tweet_tknzr = TweetTokenizer(strip_handles=True)\n",
    "first_tweet_lower_tt = tweet_tknzr.tokenize(first_tweet_lower)\n",
    "first_tweet_lower_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. i have a 3g iphone . after 3 hrs tweeting at #rise_austin , it was dead ! i need to upgrade . plugin stations at #sxsw .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn tokenized words back into tweet\n",
    "first_tweet_lower_tweet = \" \".join(first_tweet_lower_tt)\n",
    "first_tweet_lower_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have',\n",
       " '3g',\n",
       " 'iphone',\n",
       " 'after',\n",
       " 'hrs',\n",
       " 'tweeting',\n",
       " 'at',\n",
       " 'rise_austin',\n",
       " 'it',\n",
       " 'was',\n",
       " 'dead',\n",
       " 'need',\n",
       " 'to',\n",
       " 'upgrade',\n",
       " 'plugin',\n",
       " 'stations',\n",
       " 'at',\n",
       " 'sxsw']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use regexptokenizer\n",
    "pattern = r\"(?u)\\w{2,}\" # select all words with 2 or more characters\n",
    "         #r\"(?u)\\b\\w\\w+\\b\"\n",
    "         #r'\\w+'     <-REMOVES PUNCTUATION\n",
    "regexp_tknzr = RegexpTokenizer(pattern)\n",
    "first_tweet_regexp = regexp_tknzr.tokenize(first_tweet_lower_tweet)\n",
    "first_tweet_regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3g',\n",
       " 'iphone',\n",
       " 'hrs',\n",
       " 'tweeting',\n",
       " 'rise_austin',\n",
       " 'dead',\n",
       " 'need',\n",
       " 'upgrade',\n",
       " 'plugin',\n",
       " 'stations',\n",
       " 'sxsw']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of stopwords in English\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "#remove stopwords\n",
    "first_tweet_sw_removed = [word for word in first_tweet_regexp if word not in stopwords_list]\n",
    "first_tweet_sw_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3g',\n",
       " 'iphone',\n",
       " 'hr',\n",
       " 'tweeting',\n",
       " 'rise_austin',\n",
       " 'dead',\n",
       " 'need',\n",
       " 'upgrade',\n",
       " 'plugin',\n",
       " 'station',\n",
       " 'sxsw']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create lemma object\n",
    "lemma = WordNetLemmatizer()\n",
    "first_tweet_lemma = [lemma.lemmatize(token) for token in first_tweet_sw_removed]\n",
    "first_tweet_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering/Prepping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet             Product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "          Sentiment  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns = {'tweet_text': 'Tweet', \n",
    "                         'emotion_in_tweet_is_directed_at': 'Product', \n",
    "                         'is_there_an_emotion_directed_at_a_brand_or_product': 'Sentiment'})\n",
    "df.head() #Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9093, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x8cÏ¡\\x8eÏà\\x8aü_\\x8b\\x81Ê\\x8b\\x81Î\\x8b\\x81Ò\\x8b\\x81£\\x8b\\x81Á\\x8bââ\\x8b\\x81_\\x8b\\x81£\\x8b\\x81\\x8f\\x8bâ_\\x8bÛâRT @mention Google Tests \\x89ÛÏCheck-in Offers\\x89Û\\x9d At #SXSW {link}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweet'].iloc[9092]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweet'].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([6, 9092], inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df['Tweet'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet           0\n",
       "Product      5787\n",
       "Sentiment       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5374\n",
       "Positive emotion                      2970\n",
       "Negative emotion                       569\n",
       "I can't tell                           156\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   iPhone\n",
       "1       iPad or iPhone App\n",
       "2                     iPad\n",
       "3       iPad or iPhone App\n",
       "4                   Google\n",
       "               ...        \n",
       "9087             Undefined\n",
       "9088                  iPad\n",
       "9089             Undefined\n",
       "9090             Undefined\n",
       "9091             Undefined\n",
       "Name: Product, Length: 9069, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Product.fillna(\"Undefined\", inplace = True)\n",
    "df[\"Product\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Undefined                          5787\n",
       "iPad                                945\n",
       "Apple                               659\n",
       "iPad or iPhone App                  469\n",
       "Google                              428\n",
       "iPhone                              296\n",
       "Other Google product or service     293\n",
       "Android App                          80\n",
       "Android                              77\n",
       "Other Apple product or service       35\n",
       "Name: Product, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9069 entries, 0 to 9091\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Tweet      9069 non-null   object\n",
      " 1   Product    9069 non-null   object\n",
      " 2   Sentiment  9069 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 283.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple           5361\n",
       "Google          2756\n",
       "Undetermined     739\n",
       "Both             213\n",
       "Name: Brand, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_brand(Product, Tweet): #Building function to determine Brand\n",
    "    brand = 'Undetermined' #Labeling brand as Undetermined\n",
    "    if ((Product.lower().__contains__('google')) or (Product.lower().__contains__('android'))): #Labeling Google\n",
    "        brand = 'Google' #Unless tweet contains google or android\n",
    "    elif ((Product.lower().__contains__('apple')) or (Product.lower().__contains__('ip'))): #Labeling Apple\n",
    "        brand = 'Apple' #Unless tweet contains apple or ip\n",
    "    \n",
    "    if (brand == 'Undetermined'): \n",
    "        lower_tweet = Tweet.lower() #Making tweet lowercase\n",
    "        is_google = (lower_tweet.__contains__('google')) or (lower_tweet.__contains__('android')) #Undetermined google\n",
    "        is_apple = (lower_tweet.__contains__('apple')) or (lower_tweet.__contains__('ip')) #Undetermined apple\n",
    "        \n",
    "        if (is_google and is_apple): #if it has both identifiers in the tweet\n",
    "            brand = 'Both' #Labeling brand as both\n",
    "        elif (is_google):\n",
    "            brand = 'Google' #Labeling brand as Google\n",
    "        elif (is_apple):\n",
    "            brand = 'Apple' #Labeling brand as Apple\n",
    "    \n",
    "    return brand\n",
    "\n",
    "df['Brand'] = df.apply(lambda x: find_brand(x['Product'], x['Tweet']), axis = 1) #Applying function to column\n",
    "df['Brand'].value_counts() #Reviewing value counts of each class within brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = df['Sentiment'].apply(lambda x: 1 if x == \"Positive emotion\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet             Product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "   Sentiment   Brand  \n",
       "0          0   Apple  \n",
       "1          1   Apple  \n",
       "2          1   Apple  \n",
       "3          0   Apple  \n",
       "4          1  Google  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Apple and Google DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame of Tweets only about Apple\n",
    "apple_df = df[df['Brand']=='Apple']\n",
    "#Create a DataFrame of Tweets only about Google\n",
    "google_df = df[df['Brand']==\"Google\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet             Product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "5  @teachntech00 New iPad Apps For #SpeechTherapy...           Undefined   \n",
       "\n",
       "   Sentiment  Brand  \n",
       "0          0  Apple  \n",
       "1          1  Apple  \n",
       "2          1  Apple  \n",
       "3          0  Apple  \n",
       "5          0  Apple  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>Android</td>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Excited to meet the @samsungmobileus at #sxsw ...</td>\n",
       "      <td>Android</td>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Find &amp;amp; Start Impromptu Parties at #SXSW Wi...</td>\n",
       "      <td>Android App</td>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Foursquare ups the game, just in time for #SXS...</td>\n",
       "      <td>Android App</td>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet      Product  Sentiment  \\\n",
       "4   @sxtxstate great stuff on Fri #SXSW: Marissa M...       Google          1   \n",
       "7   #SXSW is just starting, #CTIA is around the co...      Android          1   \n",
       "10  Excited to meet the @samsungmobileus at #sxsw ...      Android          1   \n",
       "11  Find &amp; Start Impromptu Parties at #SXSW Wi...  Android App          1   \n",
       "12  Foursquare ups the game, just in time for #SXS...  Android App          1   \n",
       "\n",
       "     Brand  \n",
       "4   Google  \n",
       "7   Google  \n",
       "10  Google  \n",
       "11  Google  \n",
       "12  Google  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = apple_df[['Tweet']]\n",
    "# y = apple_df['Sentiment']\n",
    "# X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing 2 Train_Test_Splits on whole Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the train-test split before the Data Preprocessing prevents data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the X DataFrame with only the tweets\n",
    "X = df[['Tweet']]\n",
    "#Create the y Series with only the sentiments, 1 for Positive, 0 for not Positive\n",
    "y = df['Sentiment']\n",
    "#First train test split\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "#Second train test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.67636\n",
       "1    0.32364\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Baseline Understanding\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If we did a multi class\n",
    "# dict_sent = {'No emotion toward brand or product':1, \n",
    "#              'Positive emotion':2,\n",
    "#              'Negative emotion':0,\n",
    "#              \"I can't tell\": 1}\n",
    "# df['Sentiment'] = df['Sentiment'].map(dict_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Preprocess targets\n",
    "# y_train = y_train.apply(lambda x: 1 if x == \"Positive emotion\" else 0)\n",
    "# y_val = y_val.apply(lambda x: 1 if x == \"Positive emotion\" else 0)\n",
    "# y_test = y_test.apply(lambda x: 1 if x == \"Positive emotion\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>@mention Can we make you an iPhone case with T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5632</th>\n",
       "      <td>RT @mention Come party down with @mention &amp;amp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>#winning #winning - just gave away 5 red mophi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5799</th>\n",
       "      <td>RT @mention google &amp;amp; facebook have an offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>Rumor of Google launching their new social net...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet\n",
       "2324  @mention Can we make you an iPhone case with T...\n",
       "5632  RT @mention Come party down with @mention &amp...\n",
       "1751  #winning #winning - just gave away 5 red mophi...\n",
       "5799  RT @mention google &amp; facebook have an offi...\n",
       "3339  Rumor of Google launching their new social net..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6121, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2041, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate necessary tools\n",
    "tokenizer = RegexpTokenizer(r\"(?u)\\w{3,}\")\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "stopwords_list.append(\"sxsw\") #remove sxsw because it's the hashtag for the event\n",
    "lemma = WordNetLemmatizer()\n",
    "tweet_tknzr = TweetTokenizer(strip_handles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'sxsw']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(text):\n",
    "    #Use TweetTokenizer object to remove the handles from the Tweet\n",
    "    #TweetTokenizer also puts each punctuation as it's own token\n",
    "    no_handle = tweet_tknzr.tokenize(text)\n",
    "    #Join the list of non-handle tokens back together\n",
    "    tweet = \" \".join(no_handle) \n",
    "    #remove http websites, hashtag sign, any words in curly brackets,\n",
    "        #any words with ampersand in front, www dot com websites, links,\n",
    "        #videos, and non-english characters\n",
    "    clean = re.sub(\"(https?:\\/\\/\\S+) \\\n",
    "                   |(#[A-Za-z0-9_]+) \\\n",
    "                   |(\\{([a-zA-Z].+)\\}) \\\n",
    "                   |(&[a-z]+;) \\\n",
    "                   |(www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com))\\\n",
    "                   |({link})\\\n",
    "                   |(\\[video\\])\\\n",
    "                   |([^\\x00-\\x7F]+\\ *(?:[^\\x00-\\x7F]| )*)\",\" \", tweet)\n",
    "    #Turn all the tokens lowercase\n",
    "    lower = clean.lower()\n",
    "    #Only include words with 3 or more characters\n",
    "    token_list = tokenizer.tokenize(lower)\n",
    "    #Remove stopwords\n",
    "    stopwords_removed=[token for token in token_list if token not in stopwords_list]\n",
    "    #Lemmatize the remaining word tokens\n",
    "    lemma_list = [lemma.lemmatize(token) for token in stopwords_removed]\n",
    "    #Turn the lemma list into a string for the Vectorizer\n",
    "    cleaned_string = \" \".join(lemma_list) \n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make iphone case ttye time want show support'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "clean_tweets(X_train['Tweet'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_51772/1064438169.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['Tweet'] = X_train['Tweet'].apply(lambda x: clean_tweets(x))\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_51772/1064438169.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val['Tweet'] = X_val['Tweet'].apply(lambda x: clean_tweets(x))\n",
      "C:\\Users\\david\\AppData\\Local\\Temp/ipykernel_51772/1064438169.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['Tweet'] = X_test['Tweet'].apply(lambda x: clean_tweets(x))\n"
     ]
    }
   ],
   "source": [
    "#Apply our clean_tweets function to X_train, X_val, X_test\n",
    "X_train['Tweet'] = X_train['Tweet'].apply(lambda x: clean_tweets(x))\n",
    "X_val['Tweet'] = X_val['Tweet'].apply(lambda x: clean_tweets(x))\n",
    "X_test['Tweet'] = X_test['Tweet'].apply(lambda x: clean_tweets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>make iphone case ttye time want show support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5632</th>\n",
       "      <td>come party google tonight link band food art i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>winning winning gave away red mophie juice pac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5799</th>\n",
       "      <td>google facebook official death policy vast maj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>rumor google launching new social network call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5702</th>\n",
       "      <td>even security guard austin enjoy ipad time link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8604</th>\n",
       "      <td>attending want explore austin check austin lin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7836</th>\n",
       "      <td>apple popup store link gonnagetanipad2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>putting pop apple store smart talk understandi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3536</th>\n",
       "      <td>thought path iphone photo app iphoneography mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6121 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet\n",
       "2324       make iphone case ttye time want show support\n",
       "5632  come party google tonight link band food art i...\n",
       "1751  winning winning gave away red mophie juice pac...\n",
       "5799  google facebook official death policy vast maj...\n",
       "3339  rumor google launching new social network call...\n",
       "...                                                 ...\n",
       "5702    even security guard austin enjoy ipad time link\n",
       "8604  attending want explore austin check austin lin...\n",
       "7836             apple popup store link gonnagetanipad2\n",
       "7504  putting pop apple store smart talk understandi...\n",
       "3536  thought path iphone photo app iphoneography mo...\n",
       "\n",
       "[6121 rows x 1 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>hootsuite mobile update iphone blackberry andr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>morning hearing google circle today link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>great location choice nice timing ipad launch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>win ipad via link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>launching product plenty else join h4ckers 80 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>racing around best fueling great local fare ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>omg still line new ipad dieing hunger else line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7735</th>\n",
       "      <td>hour popup apple store lone security guard enj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8211</th>\n",
       "      <td>great app interface example moma target flipbo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4517</th>\n",
       "      <td>global android activation mapped animated link...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2041 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet\n",
       "891   hootsuite mobile update iphone blackberry andr...\n",
       "4198           morning hearing google circle today link\n",
       "2164  great location choice nice timing ipad launch ...\n",
       "1885                                  win ipad via link\n",
       "4700  launching product plenty else join h4ckers 80 ...\n",
       "...                                                 ...\n",
       "1033  racing around best fueling great local fare ea...\n",
       "4186    omg still line new ipad dieing hunger else line\n",
       "7735  hour popup apple store lone security guard enj...\n",
       "8211  great app interface example moma target flipbo...\n",
       "4517  global android activation mapped animated link...\n",
       "\n",
       "[2041 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2324    0\n",
       "5632    1\n",
       "1751    0\n",
       "5799    0\n",
       "3339    0\n",
       "       ..\n",
       "5702    1\n",
       "8604    0\n",
       "7836    0\n",
       "7504    1\n",
       "3536    0\n",
       "Name: Sentiment, Length: 6121, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891     0\n",
       "4198    0\n",
       "2164    1\n",
       "1885    1\n",
       "4700    0\n",
       "       ..\n",
       "1033    0\n",
       "4186    1\n",
       "7735    0\n",
       "8211    1\n",
       "4517    0\n",
       "Name: Sentiment, Length: 2041, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DON'T NEED BECAUSE I ADDED A LINE TO THE CLEAN_TWEETS FUNCTION\n",
    "\n",
    "# X_train[\"Tweet\"] = X_train[\"Tweet\"].str.join(\" \")\n",
    "# X_val[\"Tweet\"] = X_val[\"Tweet\"].str.join(\" \")\n",
    "# X_test[\"Tweet\"] = X_test[\"Tweet\"].str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6121x7144 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 56927 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vectorizer = CountVectorizer()\n",
    "c_vectorizer.fit(X_train['Tweet'])\n",
    "X_train_c_vec = c_vectorizer.transform(X_train['Tweet'])\n",
    "X_train_c_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>0310apple</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>101</th>\n",
       "      <th>106</th>\n",
       "      <th>10am</th>\n",
       "      <th>10pm</th>\n",
       "      <th>10x</th>\n",
       "      <th>10x2</th>\n",
       "      <th>...</th>\n",
       "      <th>zlf</th>\n",
       "      <th>zms</th>\n",
       "      <th>zomb</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zomg</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zynga</th>\n",
       "      <th>zzzs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5632</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5799</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5702</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8604</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7836</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7504</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3536</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6121 rows × 7144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      000  0310apple  100  1000  101  106  10am  10pm  10x  10x2  ...  zlf  \\\n",
       "2324    0          0    0     0    0    0     0     0    0     0  ...    0   \n",
       "5632    0          0    0     0    0    0     0     0    0     0  ...    0   \n",
       "1751    0          0    0     0    0    0     0     0    0     0  ...    0   \n",
       "5799    0          0    0     0    0    0     0     0    0     0  ...    0   \n",
       "3339    0          0    0     0    0    0     0     0    0     0  ...    0   \n",
       "...   ...        ...  ...   ...  ...  ...   ...   ...  ...   ...  ...  ...   \n",
       "5702    0          0    0     0    0    0     0     0    0     0  ...    0   \n",
       "8604    0          0    0     0    0    0     0     0    0     0  ...    0   \n",
       "7836    0          0    0     0    0    0     0     0    0     0  ...    0   \n",
       "7504    0          0    0     0    0    0     0     0    0     0  ...    0   \n",
       "3536    0          0    0     0    0    0     0     0    0     0  ...    0   \n",
       "\n",
       "      zms  zomb  zombie  zomg  zone  zoom  zuckerberg  zynga  zzzs  \n",
       "2324    0     0       0     0     0     0           0      0     0  \n",
       "5632    0     0       0     0     0     0           0      0     0  \n",
       "1751    0     0       0     0     0     0           0      0     0  \n",
       "5799    0     0       0     0     0     0           0      0     0  \n",
       "3339    0     0       0     0     0     0           0      0     0  \n",
       "...   ...   ...     ...   ...   ...   ...         ...    ...   ...  \n",
       "5702    0     0       0     0     0     0           0      0     0  \n",
       "8604    0     0       0     0     0     0           0      0     0  \n",
       "7836    0     0       0     0     0     0           0      0     0  \n",
       "7504    0     0       0     0     0     0           0      0     0  \n",
       "3536    0     0       0     0     0     0           0      0     0  \n",
       "\n",
       "[6121 rows x 7144 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_c_vec_df = pd.DataFrame(X_train_c_vec.toarray(), columns=c_vectorizer.get_feature_names(), \n",
    "                              index=X_train.index)\n",
    "X_train_c_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Sanity Check\n",
    "# X_train_c_vec_df['sxsw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_c_vec = c_vectorizer.transform(X_val['Tweet'])\n",
    "X_val_c_vec_df = pd.DataFrame(X_val_c_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6121, 7144)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(X_train['Tweet'])\n",
    "X_train_tfidf_vec = tfidf_vectorizer.transform(X_train['Tweet'])\n",
    "X_val_tfidf_vec = tfidf_vectorizer.transform(X_val['Tweet'])\n",
    "X_train_tfidf_vec_df = pd.DataFrame(X_train_tfidf_vec.toarray())\n",
    "X_val_tfidf_vec_df = pd.DataFrame(X_val_tfidf_vec.toarray())\n",
    "X_train_tfidf_vec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Logistic Regression Model w/ Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8923378532919458\n",
      "0.7403233708966193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train_c_vec_df, y_train)\n",
    "print(lr.score(X_train_c_vec_df, y_train))\n",
    "print(lr.score(X_val_c_vec_df, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Logistic Regression Model w/ Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7988890704133311\n",
      "0.7295443410093092\n"
     ]
    }
   ],
   "source": [
    "lr_2 = LogisticRegression()\n",
    "lr_2.fit(X_train_tfidf_vec_df, y_train)\n",
    "print(lr_2.score(X_train_tfidf_vec_df, y_train))\n",
    "print(lr_2.score(X_val_tfidf_vec_df, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes w/ Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857049501715406\n",
      "0.7143557079862812\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_c_vec_df, y_train)\n",
    "print(naive_bayes.score(X_train_c_vec_df, y_train))\n",
    "print(naive_bayes.score(X_val_c_vec_df, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes w/ Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7903937265152753\n",
      "0.7128858402743753\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_2 = MultinomialNB()\n",
    "naive_bayes_2.fit(X_train_tfidf_vec_df, y_train)\n",
    "print(naive_bayes_2.score(X_train_tfidf_vec_df, y_train))\n",
    "print(naive_bayes_2.score(X_val_tfidf_vec_df, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes w/ Tuned Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vectorizer_2 = CountVectorizer(max_df=.99,min_df=.003, max_features=1000)\n",
    "    #max_df=.95,  # removes words that appear in more than 95% of docs\n",
    "    #min_df=2     # removes words that appear 2 or fewer times\n",
    "c_vectorizer_2.fit(X_train['Tweet'])\n",
    "X_train_c_vec_2 = c_vectorizer_2.transform(X_train['Tweet'])\n",
    "X_val_c_vec_2 = c_vectorizer_2.transform(X_val['Tweet'])\n",
    "X_train_c_vec_df_2 = pd.DataFrame(X_train_c_vec_2.toarray())\n",
    "X_val_c_vec_df_2 = pd.DataFrame(X_val_c_vec_2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes with tuned count vectorizer\n",
      "0.7319065512171213\n",
      "0.7109260166585007\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_3 = MultinomialNB(alpha=.1)\n",
    "naive_bayes_3.fit(X_train_c_vec_df_2, y_train)\n",
    "print(\"naive bayes with tuned count vectorizer\")\n",
    "print(naive_bayes_3.score(X_train_c_vec_df_2, y_train))\n",
    "print(naive_bayes_3.score(X_val_c_vec_df_2, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHOLE DATASET\n",
    "# naive bayes with tuned count vectorizer\n",
    "# 0.7582094429014867\n",
    "# 0.705046545810877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_2 = TfidfVectorizer(max_df=.99,min_df=0.005, max_features=1000)\n",
    "tfidf_vectorizer_2.fit(X_train['Tweet'])\n",
    "X_train_tfidf_vec_2 = tfidf_vectorizer_2.transform(X_train['Tweet'])\n",
    "X_val_tfidf_vec_2 = tfidf_vectorizer_2.transform(X_val['Tweet'])\n",
    "X_train_tfidf_vec_df_2 = pd.DataFrame(X_train_tfidf_vec_2.toarray())\n",
    "X_val_tfidf_vec_df_2 = pd.DataFrame(X_val_tfidf_vec_2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes with tuned tfidf\n",
      "0.7162228394053259\n",
      "0.7011268985791279\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_4 = MultinomialNB()\n",
    "naive_bayes_4.fit(X_train_tfidf_vec_df_2, y_train)\n",
    "print(\"naive bayes with tuned tfidf\")\n",
    "print(naive_bayes_4.score(X_train_tfidf_vec_df_2, y_train))\n",
    "print(naive_bayes_4.score(X_val_tfidf_vec_df_2, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHOLE DATASET\n",
    "# naive bayes with tuned tfidf\n",
    "# 0.7162228394053259\n",
    "# 0.7011268985791279"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score \n",
    "from sklearn.metrics import roc_auc_score, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate function\n",
    "def evaluate(estimator, X_tr, X_te, y_tr, y_te, roc_auc='skip'):\n",
    "    '''\n",
    "    orginial function at : \n",
    "    (https://github.com/lindseyberlin/Cat-in-the-Dat-Project/blob/main/notebooks/Lindsey/EDA-Initial-Models.ipynb)\n",
    "    Evaluation function to show a few scores for both the train and test set\n",
    "    Also shows a confusion matrix for the test set\n",
    "    \n",
    "    roc_auc allows you to set how to calculate the roc_auc score: \n",
    "    'dec' for decision_function or 'proba' for predict_proba \n",
    "    If roc_auc == 'skip', then it ignores calculating the roc_auc_score\n",
    "    \n",
    "    Inputs:\n",
    "        estimator: a fit sklearn-style model or pipeline\n",
    "        X_tr: array or pandas dataframe\n",
    "            training input variables\n",
    "        X_te: array or pandas dataframe\n",
    "            testing input variables\n",
    "        y_tr: array or pandas series\n",
    "            training output variable\n",
    "        y_te: array or pandas series\n",
    "            testing output variable\n",
    "        roc_auc: str\n",
    "            'skip': default, skips calculating roc_auc\n",
    "            'dec': use decision_function to calculate roc_auc\n",
    "            'proba': use predict_proba to calculate roc_auc\n",
    "            \n",
    "    '''\n",
    "    # Grab predictions\n",
    "    tr_preds = estimator.predict(X_tr)\n",
    "    te_preds = estimator.predict(X_te)\n",
    "    \n",
    "    # output needed for roc_auc_score\n",
    "    if roc_auc == 'skip': # skips calculating the roc_auc_score\n",
    "        train_out = False\n",
    "        test_out = False\n",
    "    elif roc_auc == 'dec': \n",
    "        train_out = estimator.decision_function(X_train)\n",
    "        test_out = estimator.decision_function(X_test)\n",
    "    elif roc_auc == 'proba': \n",
    "        train_out = estimator.predict_proba(X_train)[:, 1] # proba for the 1 class\n",
    "        test_out = estimator.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        raise Exception(\"The value for roc_auc should be 'skip', 'dec' or 'proba'\")    \n",
    "    \n",
    "    print(\"Training Scores:\")\n",
    "    print(f\"Train Accuracy: {accuracy_score(y_tr, tr_preds)}\")\n",
    "    print(f\"Train Precision: {precision_score(y_tr, tr_preds)}\")\n",
    "    print(f\"Train Recall: {recall_score(y_tr, tr_preds)}\")\n",
    "    print(f\"Train F1-Score: {f1_score(y_tr, tr_preds)}\")\n",
    "    if type(train_out) == np.ndarray: # checking for roc_auc\n",
    "        print(f\"ROC-AUC: {roc_auc_score(y_train, train_out)}\")\n",
    "        \n",
    "    print(\"*\" * 10)\n",
    "    print(\"Testing Scores:\")\n",
    "    print(f\"Test Accuracy: {accuracy_score(y_te, te_preds)}\")\n",
    "    print(f\"Test Precision: {precision_score(y_te, te_preds)}\")\n",
    "    print(f\"Test Recall: {recall_score(y_te, te_preds)}\")\n",
    "    print(f\"Test F1-Score: {f1_score(y_te, te_preds)}\")\n",
    "    if type(test_out) == np.ndarray: # checking for roc_auc\n",
    "        print(f\"ROC-AUC: {roc_auc_score(y_test, test_out)}\")    \n",
    "    \n",
    "    # Plot confusion matrix for test set\n",
    "    plot_confusion_matrix(estimator, X_te, y_te, values_format='.5g', cmap=plt.cm.Oranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_tuned = Pipeline(steps=[\n",
    "    ('col_selector', ColumnSelector(cols=('Tweet'),drop_axis=True)),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"tfidf__max_df\" : [.99],\n",
    "    \"tfidf__min_df\" : [.0003,],\n",
    "    \"tfidf__max_features\" : [1000],\n",
    "    'clf__alpha': [.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(nb_tuned, param_grid, scoring = \"recall\", return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.1,\n",
       " 'tfidf__max_df': 0.99,\n",
       " 'tfidf__max_features': 1000,\n",
       " 'tfidf__min_df': 0.0003}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "Train Accuracy: 0.7647443228230681\n",
      "Train Precision: 0.775178026449644\n",
      "Train Recall: 0.38465421504290764\n",
      "Train F1-Score: 0.51417004048583\n",
      "ROC-AUC: 0.8094020611265963\n",
      "**********\n",
      "Testing Scores:\n",
      "Test Accuracy: 0.7138657520823126\n",
      "Test Precision: 0.653179190751445\n",
      "Test Recall: 0.32753623188405795\n",
      "Test F1-Score: 0.43629343629343625\n",
      "ROC-AUC: 0.7214426377398345\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(output.best_estimator_, X_train, X_val, y_train, y_val, roc_auc='proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with max iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default count vectorizer\n",
      "0.900497512437811\n",
      "0.6990049751243781\n"
     ]
    }
   ],
   "source": [
    "lr_3 = LogisticRegression(max_iter=1000)\n",
    "lr_3.fit(X_train_c_vec_df, y_train)\n",
    "print(\"default count vectorizer\")\n",
    "print(lr_3.score(X_train_c_vec_df, y_train))\n",
    "print(lr_3.score(X_val_c_vec_df, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default tfidf vectorizer\n",
      "0.8228302929795467\n",
      "0.6965174129353234\n"
     ]
    }
   ],
   "source": [
    "#lr_3 = LogisticRegression(max_iter=1000)\n",
    "lr_3.fit(X_train_tfidf_vec_df, y_train)\n",
    "print(\"default tfidf vectorizer\")\n",
    "print(lr_3.score(X_train_tfidf_vec_df, y_train))\n",
    "print(lr_3.score(X_val_tfidf_vec_df, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf vectorizer with tuned parameters\n",
      "0.7252625760088447\n",
      "0.6824212271973465\n"
     ]
    }
   ],
   "source": [
    "#lr_3 = LogisticRegression(max_iter=1000)\n",
    "lr_3.fit(X_train_tfidf_vec_df_2, y_train)\n",
    "print(\"tfidf vectorizer with tuned parameters\")\n",
    "print(lr_3.score(X_train_tfidf_vec_df_2, y_train))\n",
    "print(lr_3.score(X_val_tfidf_vec_df_2, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA w/ Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a StandardScaler Object\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_c_vec_df) #Use default count vectorizer\n",
    "X_val_scaled = scaler.transform(X_val_c_vec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1844"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to import, instantiate and fit a PCA object\n",
    "pca = PCA(n_components = .95, random_state=42)\n",
    "pca.fit(X_train_scaled)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA with n_components=0.95, default count vectorizer, and logistic regression\n",
      "0.9494195688225538\n",
      "0.6799336650082919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# Construct a pipelines\n",
    "pipe_lr = Pipeline([('pca', pca), \n",
    "                    ('lr', LogisticRegression(random_state=42, max_iter=1000))])\n",
    "pipe_lr.fit(X_train_scaled, y_train)\n",
    "print(\"PCA with n_components=0.95, default count vectorizer, and logistic regression\")\n",
    "print(pipe_lr.score(X_train_scaled, y_train))\n",
    "print(pipe_lr.score(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe_mnb = Pipeline([('pca', pca), \n",
    "#                     ('mnb', MultinomialNB())])\n",
    "# pipe_mnb.fit(X_train_scaled, y_train)\n",
    "# print(\"PCA with n_components=0.95, default count vectorizer, and naive bayes\")\n",
    "# print(pipe_lr.score(X_train_scaled, y_train))\n",
    "# print(pipe_lr.score(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I got an error about MultinomialNB not having negative values*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA w/ Tuned TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a StandardScaler Object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Fit scaler to tuned vectorized X_train and transform both X_train and X_val\n",
    "X_train_scaled = scaler.fit_transform(X_train_tfidf_vec_df_2)\n",
    "X_val_scaled = scaler.transform(X_val_tfidf_vec_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a PCA object\n",
    "pca = PCA(n_components = .99, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train_scaled)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pca', PCA(n_components=0.99, random_state=42)),\n",
       "                ('lr', LogisticRegression(max_iter=1000, random_state=42))])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Pipeline object for PCA and Logistic Regression\n",
    "pipe_lr = Pipeline([('pca', pca), \n",
    "                    ('lr', LogisticRegression(random_state=42, max_iter=1000))])\n",
    "pipe_lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA with n_components=0.99, tuned tfidf vectorizer, and naive bayes\n",
      "0.7252625760088447\n",
      "0.681592039800995\n"
     ]
    }
   ],
   "source": [
    "#Check results\n",
    "print(\"PCA with n_components=0.99, tuned tfidf vectorizer, and naive bayes\")\n",
    "print(pipe_lr.score(X_train_scaled, y_train))\n",
    "print(pipe_lr.score(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with n_components=0.99, tuned tfidf vectorizer, and naive bayes\n",
    "# 0.7250449272994609\n",
    "# 0.7099461048505634"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline and Cross Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, plot_confusion_matrix, plot_roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garrettwilliams/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/garrettwilliams/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/garrettwilliams/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/garrettwilliams/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.14137435, 0.14024305, 0.138978  , 0.13517618, 0.13740969]),\n",
       " 'score_time': array([0.01971579, 0.01860189, 0.01813698, 0.01784301, 0.01842141]),\n",
       " 'test_accuracy': array([0.74040816, 0.71486928, 0.75571895, 0.71405229, 0.73856209]),\n",
       " 'train_accuracy': array([0.90012255, 0.90095977, 0.8989177 , 0.90300184, 0.90014294]),\n",
       " 'test_precision': array([0.64468864, 0.58545455, 0.68301887, 0.5777027 , 0.64393939]),\n",
       " 'train_precision': array([0.93008641, 0.93035994, 0.92182663, 0.93977813, 0.9335443 ]),\n",
       " 'test_roc_auc': array([0.76039043, 0.72168393, 0.75111928, 0.72181965, 0.72928713]),\n",
       " 'train_roc_auc': array([0.9658115 , 0.96702756, 0.96351533, 0.96518872, 0.96585212])}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_logreg = Pipeline(steps=[\n",
    "    ('count_vectorizer', CountVectorizer()),\n",
    "    ('logreg', LogisticRegression(random_state=42))\n",
    "])\n",
    "cv = cross_validate(pipe_logreg, X_train['Tweet'], y_train, return_train_score=True, \\\n",
    "                    scoring=['accuracy', 'precision','roc_auc'])\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Naive Bayes w/ Default Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.0408442 , 0.03933597, 0.03985906, 0.03992701, 0.04186916]),\n",
       " 'score_time': array([0.01813602, 0.01813412, 0.01827288, 0.01857829, 0.01944375]),\n",
       " 'test_accuracy': array([0.70612245, 0.70506536, 0.69934641, 0.69444444, 0.70915033]),\n",
       " 'train_accuracy': array([0.78696895, 0.7945681 , 0.78742087, 0.79252604, 0.79048397]),\n",
       " 'test_recall': array([0.15365239, 0.14646465, 0.12626263, 0.13636364, 0.16161616]),\n",
       " 'train_recall': array([0.35669192, 0.38170347, 0.35772871, 0.37223975, 0.36719243]),\n",
       " 'test_roc_auc': array([0.73155094, 0.6767448 , 0.69595411, 0.70236178, 0.70092379]),\n",
       " 'train_roc_auc': array([0.88950838, 0.89780161, 0.89179296, 0.89564979, 0.89119786])}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a pipeline to use an Untuned TfidfVectorizer() and MultinomialNB()\n",
    "pipe_nb = Pipeline(steps=[\n",
    "    ('tfidf_vectorizer', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "#Cross validate\n",
    "cv = cross_validate(pipe_nb, X_train['Tweet'], y_train, return_train_score=True, \\\n",
    "                    scoring=['accuracy', 'recall','roc_auc'])\n",
    "#See the results\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7028257969854609"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get mean accuracy for validation set\n",
    "cv['test_accuracy'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Naive Bayes w/ Tuned Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.04241419, 0.04056215, 0.03925681, 0.03921103, 0.039891  ]),\n",
       " 'score_time': array([0.01800704, 0.01839113, 0.01804519, 0.01959586, 0.01807213]),\n",
       " 'test_accuracy': array([0.71265306, 0.7001634 , 0.70588235, 0.68954248, 0.70343137]),\n",
       " 'train_accuracy': array([0.71486928, 0.71880743, 0.71554013, 0.7177864 , 0.71860323]),\n",
       " 'test_recall': array([0.19647355, 0.18181818, 0.16414141, 0.17676768, 0.16666667]),\n",
       " 'train_recall': array([0.19318182, 0.20694006, 0.18927445, 0.21514196, 0.20315457]),\n",
       " 'test_roc_auc': array([0.69983816, 0.65763309, 0.68208352, 0.66534762, 0.6631487 ]),\n",
       " 'train_roc_auc': array([0.72634787, 0.73428142, 0.72852908, 0.73547677, 0.73252431])}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a pipeline to use an Tuned TfidfVectorizer() and MultinomialNB()\n",
    "pipe_nb_tuned = Pipeline(steps=[\n",
    "    ('tfidf_vectorizer_tuned', TfidfVectorizer(max_df=.99,min_df=0.005, max_features=1000)),\n",
    "    ('nb_tuned', MultinomialNB())\n",
    "])\n",
    "#Cross validate\n",
    "cv = cross_validate(pipe_nb_tuned, X_train['Tweet'], y_train, return_train_score=True, \\\n",
    "                    scoring=['accuracy', 'recall','roc_auc'])\n",
    "#See the results\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([1.35563898, 1.32724214, 1.3579073 , 1.32797313, 1.31642294]),\n",
       " 'score_time': array([0.09072995, 0.08811212, 0.09062457, 0.0879178 , 0.08778   ]),\n",
       " 'test_accuracy': array([0.71510204, 0.71568627, 0.71650327, 0.72058824, 0.70915033]),\n",
       " 'train_accuracy': array([0.95118464, 0.94853992, 0.94894834, 0.94813151, 0.94935675]),\n",
       " 'test_precision': array([0.59677419, 0.59022556, 0.59760956, 0.608     , 0.57936508]),\n",
       " 'train_precision': array([0.96030116, 0.95997239, 0.96450939, 0.95676047, 0.96650384]),\n",
       " 'test_roc_auc': array([0.71998169, 0.71112087, 0.70720185, 0.71120474, 0.70237856]),\n",
       " 'train_roc_auc': array([0.99131925, 0.99003833, 0.99008938, 0.98925492, 0.98986803])}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rfc = Pipeline(steps=[\n",
    "    ('tfidf_vectorizer', TfidfVectorizer(max_df=.99,min_df=0.005, max_features=1000)),\n",
    "    ('rfc', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "cv = cross_validate(pipe_rfc, X_train['Tweet'], y_train, return_train_score=True, \\\n",
    "                    scoring=['accuracy', 'precision','roc_auc'])\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('tfidf_vectorizer',\n",
       "                                        TfidfVectorizer(max_df=0.99,\n",
       "                                                        max_features=1000,\n",
       "                                                        min_df=0.005)),\n",
       "                                       ('rfc',\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             param_grid={'rfc__class_weight': ['balanced'],\n",
       "                         'rfc__max_depth': [25, 50, 100],\n",
       "                         'rfc__min_samples_leaf': [1, 3, 5],\n",
       "                         'rfc__n_estimators': [500, 1000, 1500],\n",
       "                         'rfc__random_state': [42]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_rfc = {\n",
    "    \"rfc__max_depth\" :[25, 50, 100],\n",
    "    \"rfc__min_samples_leaf\" : [1, 3, 5],\n",
    "    \"rfc__n_estimators\": [500, 1000, 1500],\n",
    "    \"rfc__class_weight\" :['balanced'],\n",
    "    \"rfc__random_state\":[42]\n",
    "}\n",
    "grid_rfc = GridSearchCV(estimator = pipe_rfc, param_grid=pg_rfc, scoring='accuracy',\n",
    "                        return_train_score = True)\n",
    "grid_rfc.fit(X_train['Tweet'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_rfc__class_weight</th>\n",
       "      <th>param_rfc__max_depth</th>\n",
       "      <th>param_rfc__min_samples_leaf</th>\n",
       "      <th>param_rfc__n_estimators</th>\n",
       "      <th>param_rfc__random_state</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.318658</td>\n",
       "      <td>0.058411</td>\n",
       "      <td>0.104045</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.703673</td>\n",
       "      <td>0.692810</td>\n",
       "      <td>0.695261</td>\n",
       "      <td>0.693627</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.692369</td>\n",
       "      <td>0.008840</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.573491</td>\n",
       "      <td>0.096270</td>\n",
       "      <td>0.190555</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.702041</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>0.695261</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>0.693513</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.840107</td>\n",
       "      <td>0.197343</td>\n",
       "      <td>0.282089</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.701224</td>\n",
       "      <td>0.687092</td>\n",
       "      <td>0.702614</td>\n",
       "      <td>0.695261</td>\n",
       "      <td>0.681373</td>\n",
       "      <td>0.693513</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.624615</td>\n",
       "      <td>0.020397</td>\n",
       "      <td>0.093181</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.686531</td>\n",
       "      <td>0.678105</td>\n",
       "      <td>0.681373</td>\n",
       "      <td>0.682190</td>\n",
       "      <td>0.658497</td>\n",
       "      <td>0.677339</td>\n",
       "      <td>0.009797</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.238793</td>\n",
       "      <td>0.054916</td>\n",
       "      <td>0.177897</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.692245</td>\n",
       "      <td>0.673203</td>\n",
       "      <td>0.687092</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>0.662582</td>\n",
       "      <td>0.679789</td>\n",
       "      <td>0.010623</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.739683</td>\n",
       "      <td>0.064339</td>\n",
       "      <td>0.270101</td>\n",
       "      <td>0.023248</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.693061</td>\n",
       "      <td>0.670752</td>\n",
       "      <td>0.687092</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.679789</td>\n",
       "      <td>0.011646</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.456939</td>\n",
       "      <td>0.015529</td>\n",
       "      <td>0.089320</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.679184</td>\n",
       "      <td>0.667484</td>\n",
       "      <td>0.678105</td>\n",
       "      <td>0.678105</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.672928</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.861272</td>\n",
       "      <td>0.034160</td>\n",
       "      <td>0.169892</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.683265</td>\n",
       "      <td>0.665033</td>\n",
       "      <td>0.687092</td>\n",
       "      <td>0.673203</td>\n",
       "      <td>0.660131</td>\n",
       "      <td>0.673745</td>\n",
       "      <td>0.010299</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.256708</td>\n",
       "      <td>0.030177</td>\n",
       "      <td>0.247809</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.684898</td>\n",
       "      <td>0.660948</td>\n",
       "      <td>0.688725</td>\n",
       "      <td>0.674837</td>\n",
       "      <td>0.659314</td>\n",
       "      <td>0.673744</td>\n",
       "      <td>0.012017</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.277533</td>\n",
       "      <td>0.067883</td>\n",
       "      <td>0.135058</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.706939</td>\n",
       "      <td>0.705065</td>\n",
       "      <td>0.710784</td>\n",
       "      <td>0.709967</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>0.706257</td>\n",
       "      <td>0.004379</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.576495</td>\n",
       "      <td>0.056847</td>\n",
       "      <td>0.261771</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.708571</td>\n",
       "      <td>0.704248</td>\n",
       "      <td>0.711601</td>\n",
       "      <td>0.709967</td>\n",
       "      <td>0.702614</td>\n",
       "      <td>0.707401</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.737133</td>\n",
       "      <td>0.172062</td>\n",
       "      <td>0.389302</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.706939</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>0.707516</td>\n",
       "      <td>0.706699</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>0.703643</td>\n",
       "      <td>0.004183</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.290345</td>\n",
       "      <td>0.013868</td>\n",
       "      <td>0.115204</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.695510</td>\n",
       "      <td>0.690359</td>\n",
       "      <td>0.685458</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.674837</td>\n",
       "      <td>0.686488</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.540483</td>\n",
       "      <td>0.033923</td>\n",
       "      <td>0.221917</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.694694</td>\n",
       "      <td>0.688725</td>\n",
       "      <td>0.687908</td>\n",
       "      <td>0.687908</td>\n",
       "      <td>0.669935</td>\n",
       "      <td>0.685834</td>\n",
       "      <td>0.008346</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.789670</td>\n",
       "      <td>0.057322</td>\n",
       "      <td>0.340070</td>\n",
       "      <td>0.021676</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.688725</td>\n",
       "      <td>0.684641</td>\n",
       "      <td>0.688725</td>\n",
       "      <td>0.670752</td>\n",
       "      <td>0.685344</td>\n",
       "      <td>0.007863</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.989135</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>0.109460</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.689796</td>\n",
       "      <td>0.674020</td>\n",
       "      <td>0.675654</td>\n",
       "      <td>0.674837</td>\n",
       "      <td>0.657680</td>\n",
       "      <td>0.674397</td>\n",
       "      <td>0.010183</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.948893</td>\n",
       "      <td>0.032455</td>\n",
       "      <td>0.209460</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.689796</td>\n",
       "      <td>0.671569</td>\n",
       "      <td>0.678105</td>\n",
       "      <td>0.677288</td>\n",
       "      <td>0.662582</td>\n",
       "      <td>0.675868</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.881535</td>\n",
       "      <td>0.042614</td>\n",
       "      <td>0.309699</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.692245</td>\n",
       "      <td>0.670752</td>\n",
       "      <td>0.674837</td>\n",
       "      <td>0.675654</td>\n",
       "      <td>0.662582</td>\n",
       "      <td>0.675214</td>\n",
       "      <td>0.009694</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.203749</td>\n",
       "      <td>0.024041</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.697959</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.703431</td>\n",
       "      <td>0.706699</td>\n",
       "      <td>0.703431</td>\n",
       "      <td>0.701520</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12.346109</td>\n",
       "      <td>0.076506</td>\n",
       "      <td>0.324994</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.697959</td>\n",
       "      <td>0.701797</td>\n",
       "      <td>0.700980</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.701797</td>\n",
       "      <td>0.701683</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18.480057</td>\n",
       "      <td>0.111807</td>\n",
       "      <td>0.483685</td>\n",
       "      <td>0.005899</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.698776</td>\n",
       "      <td>0.702614</td>\n",
       "      <td>0.697712</td>\n",
       "      <td>0.706699</td>\n",
       "      <td>0.703431</td>\n",
       "      <td>0.701847</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.815874</td>\n",
       "      <td>0.033027</td>\n",
       "      <td>0.132577</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.688980</td>\n",
       "      <td>0.687908</td>\n",
       "      <td>0.690359</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.677288</td>\n",
       "      <td>0.686162</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.583226</td>\n",
       "      <td>0.061434</td>\n",
       "      <td>0.255994</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.688980</td>\n",
       "      <td>0.682190</td>\n",
       "      <td>0.690359</td>\n",
       "      <td>0.687908</td>\n",
       "      <td>0.678922</td>\n",
       "      <td>0.685672</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.360275</td>\n",
       "      <td>0.109339</td>\n",
       "      <td>0.379568</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.692245</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>0.688725</td>\n",
       "      <td>0.690359</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.686325</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.271639</td>\n",
       "      <td>0.031224</td>\n",
       "      <td>0.119724</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.688163</td>\n",
       "      <td>0.660131</td>\n",
       "      <td>0.674020</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.667484</td>\n",
       "      <td>0.673254</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.529314</td>\n",
       "      <td>0.076020</td>\n",
       "      <td>0.231062</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.697143</td>\n",
       "      <td>0.670752</td>\n",
       "      <td>0.674837</td>\n",
       "      <td>0.672386</td>\n",
       "      <td>0.667484</td>\n",
       "      <td>0.676520</td>\n",
       "      <td>0.010585</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.768269</td>\n",
       "      <td>0.110884</td>\n",
       "      <td>0.346861</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.694694</td>\n",
       "      <td>0.669118</td>\n",
       "      <td>0.674837</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.676357</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        2.318658      0.058411         0.104045        0.008570   \n",
       "1        4.573491      0.096270         0.190555        0.002927   \n",
       "2        6.840107      0.197343         0.282089        0.002942   \n",
       "3        1.624615      0.020397         0.093181        0.001943   \n",
       "4        3.238793      0.054916         0.177897        0.002050   \n",
       "5        4.739683      0.064339         0.270101        0.023248   \n",
       "6        1.456939      0.015529         0.089320        0.000478   \n",
       "7        2.861272      0.034160         0.169892        0.002214   \n",
       "8        4.256708      0.030177         0.247809        0.001791   \n",
       "9        4.277533      0.067883         0.135058        0.000474   \n",
       "10       8.576495      0.056847         0.261771        0.001782   \n",
       "11      12.737133      0.172062         0.389302        0.001426   \n",
       "12       2.290345      0.013868         0.115204        0.000478   \n",
       "13       4.540483      0.033923         0.221917        0.001201   \n",
       "14       6.789670      0.057322         0.340070        0.021676   \n",
       "15       1.989135      0.011709         0.109460        0.001683   \n",
       "16       3.948893      0.032455         0.209460        0.001994   \n",
       "17       5.881535      0.042614         0.309699        0.002136   \n",
       "18       6.203749      0.024041         0.166000        0.001573   \n",
       "19      12.346109      0.076506         0.324994        0.002842   \n",
       "20      18.480057      0.111807         0.483685        0.005899   \n",
       "21       2.815874      0.033027         0.132577        0.001857   \n",
       "22       5.583226      0.061434         0.255994        0.003229   \n",
       "23       8.360275      0.109339         0.379568        0.005834   \n",
       "24       2.271639      0.031224         0.119724        0.001922   \n",
       "25       4.529314      0.076020         0.231062        0.003380   \n",
       "26       6.768269      0.110884         0.346861        0.008413   \n",
       "\n",
       "   param_rfc__class_weight param_rfc__max_depth param_rfc__min_samples_leaf  \\\n",
       "0                 balanced                   25                           1   \n",
       "1                 balanced                   25                           1   \n",
       "2                 balanced                   25                           1   \n",
       "3                 balanced                   25                           3   \n",
       "4                 balanced                   25                           3   \n",
       "5                 balanced                   25                           3   \n",
       "6                 balanced                   25                           5   \n",
       "7                 balanced                   25                           5   \n",
       "8                 balanced                   25                           5   \n",
       "9                 balanced                   50                           1   \n",
       "10                balanced                   50                           1   \n",
       "11                balanced                   50                           1   \n",
       "12                balanced                   50                           3   \n",
       "13                balanced                   50                           3   \n",
       "14                balanced                   50                           3   \n",
       "15                balanced                   50                           5   \n",
       "16                balanced                   50                           5   \n",
       "17                balanced                   50                           5   \n",
       "18                balanced                  100                           1   \n",
       "19                balanced                  100                           1   \n",
       "20                balanced                  100                           1   \n",
       "21                balanced                  100                           3   \n",
       "22                balanced                  100                           3   \n",
       "23                balanced                  100                           3   \n",
       "24                balanced                  100                           5   \n",
       "25                balanced                  100                           5   \n",
       "26                balanced                  100                           5   \n",
       "\n",
       "   param_rfc__n_estimators param_rfc__random_state  \\\n",
       "0                      500                      42   \n",
       "1                     1000                      42   \n",
       "2                     1500                      42   \n",
       "3                      500                      42   \n",
       "4                     1000                      42   \n",
       "5                     1500                      42   \n",
       "6                      500                      42   \n",
       "7                     1000                      42   \n",
       "8                     1500                      42   \n",
       "9                      500                      42   \n",
       "10                    1000                      42   \n",
       "11                    1500                      42   \n",
       "12                     500                      42   \n",
       "13                    1000                      42   \n",
       "14                    1500                      42   \n",
       "15                     500                      42   \n",
       "16                    1000                      42   \n",
       "17                    1500                      42   \n",
       "18                     500                      42   \n",
       "19                    1000                      42   \n",
       "20                    1500                      42   \n",
       "21                     500                      42   \n",
       "22                    1000                      42   \n",
       "23                    1500                      42   \n",
       "24                     500                      42   \n",
       "25                    1000                      42   \n",
       "26                    1500                      42   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.703673   \n",
       "1   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.702041   \n",
       "2   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.701224   \n",
       "3   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.686531   \n",
       "4   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.692245   \n",
       "5   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.693061   \n",
       "6   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.679184   \n",
       "7   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.683265   \n",
       "8   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.684898   \n",
       "9   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.706939   \n",
       "10  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.708571   \n",
       "11  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.706939   \n",
       "12  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.695510   \n",
       "13  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.694694   \n",
       "14  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.693878   \n",
       "15  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.689796   \n",
       "16  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.689796   \n",
       "17  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.692245   \n",
       "18  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.697959   \n",
       "19  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.697959   \n",
       "20  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.698776   \n",
       "21  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.688980   \n",
       "22  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.688980   \n",
       "23  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.692245   \n",
       "24  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.688163   \n",
       "25  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.697143   \n",
       "26  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.694694   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.692810           0.695261           0.693627   \n",
       "1            0.691176           0.698529           0.695261   \n",
       "2            0.687092           0.702614           0.695261   \n",
       "3            0.678105           0.681373           0.682190   \n",
       "4            0.673203           0.687092           0.683824   \n",
       "5            0.670752           0.687092           0.686275   \n",
       "6            0.667484           0.678105           0.678105   \n",
       "7            0.665033           0.687092           0.673203   \n",
       "8            0.660948           0.688725           0.674837   \n",
       "9            0.705065           0.710784           0.709967   \n",
       "10           0.704248           0.711601           0.709967   \n",
       "11           0.698529           0.707516           0.706699   \n",
       "12           0.690359           0.685458           0.686275   \n",
       "13           0.688725           0.687908           0.687908   \n",
       "14           0.688725           0.684641           0.688725   \n",
       "15           0.674020           0.675654           0.674837   \n",
       "16           0.671569           0.678105           0.677288   \n",
       "17           0.670752           0.674837           0.675654   \n",
       "18           0.696078           0.703431           0.706699   \n",
       "19           0.701797           0.700980           0.705882   \n",
       "20           0.702614           0.697712           0.706699   \n",
       "21           0.687908           0.690359           0.686275   \n",
       "22           0.682190           0.690359           0.687908   \n",
       "23           0.683824           0.688725           0.690359   \n",
       "24           0.660131           0.674020           0.676471   \n",
       "25           0.670752           0.674837           0.672386   \n",
       "26           0.669118           0.674837           0.676471   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.676471         0.692369        0.008840                9  \n",
       "1            0.680556         0.693513        0.007407                8  \n",
       "2            0.681373         0.693513        0.008173                7  \n",
       "3            0.658497         0.677339        0.009797               18  \n",
       "4            0.662582         0.679789        0.010623               16  \n",
       "5            0.661765         0.679789        0.011646               17  \n",
       "6            0.661765         0.672928        0.007028               27  \n",
       "7            0.660131         0.673745        0.010299               24  \n",
       "8            0.659314         0.673744        0.012017               25  \n",
       "9            0.698529         0.706257        0.004379                2  \n",
       "10           0.702614         0.707401        0.003419                1  \n",
       "11           0.698529         0.703643        0.004183                3  \n",
       "12           0.674837         0.686488        0.006830               10  \n",
       "13           0.669935         0.685834        0.008346               13  \n",
       "14           0.670752         0.685344        0.007863               15  \n",
       "15           0.657680         0.674397        0.010183               23  \n",
       "16           0.662582         0.675868        0.008900               21  \n",
       "17           0.662582         0.675214        0.009694               22  \n",
       "18           0.703431         0.701520        0.003910                6  \n",
       "19           0.701797         0.701683        0.002531                5  \n",
       "20           0.703431         0.701847        0.003261                4  \n",
       "21           0.677288         0.686162        0.004634               12  \n",
       "22           0.678922         0.685672        0.004373               14  \n",
       "23           0.676471         0.686325        0.005665               11  \n",
       "24           0.667484         0.673254        0.009367               26  \n",
       "25           0.667484         0.676520        0.010585               19  \n",
       "26           0.666667         0.676357        0.009848               20  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_rfc.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__class_weight': 'balanced',\n",
       " 'rfc__max_depth': 50,\n",
       " 'rfc__min_samples_leaf': 1,\n",
       " 'rfc__n_estimators': 1000,\n",
       " 'rfc__random_state': 42}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
