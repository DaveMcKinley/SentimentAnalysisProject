{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a model that can rate the sentiment of a Tweet based on its content.\n",
    "\n",
    "You'll build an NLP model to analyze Twitter sentiment about Apple and Google products. The dataset comes from CrowdFlower via data.world. Human raters rated the sentiment in over 9,000 Tweets as positive, negative, or neither.\n",
    "\n",
    "Aim for a Proof of Concept There are many approaches to NLP problems - start with something simple and iterate from there. For example, you could start by limiting your analysis to positive and negative Tweets only, allowing you to build a binary classifier. Then you could add in the neutral Tweets to build out a multiclass classifier. You may also consider using some of the more advanced NLP methods in the Mod 4 Appendix.\n",
    "\n",
    "Evaluation Evaluating multiclass classifiers can be trickier than binary classifiers because there are multiple ways to mis-classify an observation, and some errors are more problematic than others. Use the business problem that your NLP project sets out to solve to inform your choice of evaluation metrics.\n",
    "\n",
    "Data: https://data.world/crowdflower/brands-and-product-emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOES THIS WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk related imports\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "judge-1377884607_tweet_product_company.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   \n",
       "9090  Google's Zeiger, a physician never reported po...   \n",
       "9091  Some Verizon iPhone customers complained their...   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "9088                            iPad   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                      Negative emotion  \n",
       "1                                      Positive emotion  \n",
       "2                                      Positive emotion  \n",
       "3                                      Negative emotion  \n",
       "4                                      Positive emotion  \n",
       "...                                                 ...  \n",
       "9088                                   Positive emotion  \n",
       "9089                 No emotion toward brand or product  \n",
       "9090                 No emotion toward brand or product  \n",
       "9091                 No emotion toward brand or product  \n",
       "9092                 No emotion toward brand or product  \n",
       "\n",
       "[9093 rows x 3 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/judge-1377884607_tweet_product_company.csv', encoding = 'unicode_escape')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_tweet = df['tweet_text'][0]\n",
    "first_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.@wesley83 i have a 3g iphone. after 3 hrs tweeting at #rise_austin, it was dead!  i need to upgrade. plugin stations at #sxsw.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lowercase\n",
    "first_tweet_lower = first_tweet.lower()\n",
    "first_tweet_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'i',\n",
       " 'have',\n",
       " 'a',\n",
       " '3g',\n",
       " 'iphone',\n",
       " '.',\n",
       " 'after',\n",
       " '3',\n",
       " 'hrs',\n",
       " 'tweeting',\n",
       " 'at',\n",
       " '#rise_austin',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'dead',\n",
       " '!',\n",
       " 'i',\n",
       " 'need',\n",
       " 'to',\n",
       " 'upgrade',\n",
       " '.',\n",
       " 'plugin',\n",
       " 'stations',\n",
       " 'at',\n",
       " '#sxsw',\n",
       " '.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweet tokenizer\n",
    "tweet_tknzr = TweetTokenizer(strip_handles=True)\n",
    "first_tweet_lower_tt = tweet_tknzr.tokenize(first_tweet_lower)\n",
    "first_tweet_lower_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. i have a 3g iphone . after 3 hrs tweeting at #rise_austin , it was dead ! i need to upgrade . plugin stations at #sxsw .'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn tokenized words back into tweet\n",
    "first_tweet_lower_tweet = \" \".join(first_tweet_lower_tt)\n",
    "first_tweet_lower_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have',\n",
       " '3g',\n",
       " 'iphone',\n",
       " 'after',\n",
       " 'hrs',\n",
       " 'tweeting',\n",
       " 'at',\n",
       " 'rise_austin',\n",
       " 'it',\n",
       " 'was',\n",
       " 'dead',\n",
       " 'need',\n",
       " 'to',\n",
       " 'upgrade',\n",
       " 'plugin',\n",
       " 'stations',\n",
       " 'at',\n",
       " 'sxsw']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use regexptokenizer\n",
    "pattern = r\"(?u)\\w{2,}\" # select all words with 2 or more characters\n",
    "         #r\"(?u)\\b\\w\\w+\\b\"\n",
    "         #r'\\w+'     <-REMOVES PUNCTUATION\n",
    "regexp_tknzr = RegexpTokenizer(pattern)\n",
    "first_tweet_regexp = regexp_tknzr.tokenize(first_tweet_lower_tweet)\n",
    "first_tweet_regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3g',\n",
       " 'iphone',\n",
       " 'hrs',\n",
       " 'tweeting',\n",
       " 'rise_austin',\n",
       " 'dead',\n",
       " 'need',\n",
       " 'upgrade',\n",
       " 'plugin',\n",
       " 'stations',\n",
       " 'sxsw']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of stopwords in English\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "#remove stopwords\n",
    "first_tweet_sw_removed = [word for word in first_tweet_regexp if word not in stopwords_list]\n",
    "first_tweet_sw_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3g',\n",
       " 'iphone',\n",
       " 'hr',\n",
       " 'tweeting',\n",
       " 'rise_austin',\n",
       " 'dead',\n",
       " 'need',\n",
       " 'upgrade',\n",
       " 'plugin',\n",
       " 'station',\n",
       " 'sxsw']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create lemma object\n",
    "lemma = WordNetLemmatizer()\n",
    "first_tweet_lemma = [lemma.lemmatize(token) for token in first_tweet_sw_removed]\n",
    "first_tweet_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering/Prepping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet             Product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "          Sentiment  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns = {'tweet_text': 'Tweet', \n",
    "                         'emotion_in_tweet_is_directed_at': 'Product', \n",
    "                         'is_there_an_emotion_directed_at_a_brand_or_product': 'Sentiment'})\n",
    "df.head() #Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9093, 3)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x8cÏ¡\\x8eÏà\\x8aü_\\x8b\\x81Ê\\x8b\\x81Î\\x8b\\x81Ò\\x8b\\x81£\\x8b\\x81Á\\x8bââ\\x8b\\x81_\\x8b\\x81£\\x8b\\x81\\x8f\\x8bâ_\\x8bÛâRT @mention Google Tests \\x89ÛÏCheck-in Offers\\x89Û\\x9d At #SXSW {link}'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweet'].iloc[9092]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweet'].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([6, 9092], inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df['Tweet'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet           0\n",
       "Product      5787\n",
       "Sentiment       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5374\n",
       "Positive emotion                      2970\n",
       "Negative emotion                       569\n",
       "I can't tell                           156\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   iPhone\n",
       "1       iPad or iPhone App\n",
       "2                     iPad\n",
       "3       iPad or iPhone App\n",
       "4                   Google\n",
       "               ...        \n",
       "9087             Undefined\n",
       "9088                  iPad\n",
       "9089             Undefined\n",
       "9090             Undefined\n",
       "9091             Undefined\n",
       "Name: Product, Length: 9069, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Product.fillna(\"Undefined\", inplace = True)\n",
    "df[\"Product\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Undefined                          5787\n",
       "iPad                                945\n",
       "Apple                               659\n",
       "iPad or iPhone App                  469\n",
       "Google                              428\n",
       "iPhone                              296\n",
       "Other Google product or service     293\n",
       "Android App                          80\n",
       "Android                              77\n",
       "Other Apple product or service       35\n",
       "Name: Product, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9069 entries, 0 to 9091\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Tweet      9069 non-null   object\n",
      " 1   Product    9069 non-null   object\n",
      " 2   Sentiment  9069 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 283.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple           5361\n",
       "Google          2756\n",
       "Undetermined     739\n",
       "Both             213\n",
       "Name: Brand, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_brand(Product, Tweet): #Building function to determine Brand\n",
    "    brand = 'Undetermined' #Labeling brand as Undetermined\n",
    "    if ((Product.lower().__contains__('google')) or (Product.lower().__contains__('android'))): #Labeling Google\n",
    "        brand = 'Google' #Unless tweet contains google or android\n",
    "    elif ((Product.lower().__contains__('apple')) or (Product.lower().__contains__('ip'))): #Labeling Apple\n",
    "        brand = 'Apple' #Unless tweet contains apple or ip\n",
    "    \n",
    "    if (brand == 'Undetermined'): \n",
    "        lower_tweet = Tweet.lower() #Making tweet lowercase\n",
    "        is_google = (lower_tweet.__contains__('google')) or (lower_tweet.__contains__('android')) #Undetermined google\n",
    "        is_apple = (lower_tweet.__contains__('apple')) or (lower_tweet.__contains__('ip')) #Undetermined apple\n",
    "        \n",
    "        if (is_google and is_apple): #if it has both identifiers in the tweet\n",
    "            brand = 'Both' #Labeling brand as both\n",
    "        elif (is_google):\n",
    "            brand = 'Google' #Labeling brand as Google\n",
    "        elif (is_apple):\n",
    "            brand = 'Apple' #Labeling brand as Apple\n",
    "    \n",
    "    return brand\n",
    "\n",
    "df['Brand'] = df.apply(lambda x: find_brand(x['Product'], x['Tweet']), axis = 1) #Applying function to column\n",
    "df['Brand'].value_counts() #Reviewing value counts of each class within brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = df['Sentiment'].apply(lambda x: 1 if x == \"Positive emotion\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet             Product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "   Sentiment   Brand  \n",
       "0          0   Apple  \n",
       "1          1   Apple  \n",
       "2          1   Apple  \n",
       "3          0   Apple  \n",
       "4          1  Google  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Apple and Google DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame of Tweets only about Apple\n",
    "apple_df = df[df['Brand']=='Apple']\n",
    "#Create a DataFrame of Tweets only about Google\n",
    "google_df = df[df['Brand']==\"Google\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet             Product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "5  @teachntech00 New iPad Apps For #SpeechTherapy...           Undefined   \n",
       "\n",
       "   Sentiment  Brand  \n",
       "0          0  Apple  \n",
       "1          1  Apple  \n",
       "2          1  Apple  \n",
       "3          0  Apple  \n",
       "5          0  Apple  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>Android</td>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Excited to meet the @samsungmobileus at #sxsw ...</td>\n",
       "      <td>Android</td>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Find &amp;amp; Start Impromptu Parties at #SXSW Wi...</td>\n",
       "      <td>Android App</td>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Foursquare ups the game, just in time for #SXS...</td>\n",
       "      <td>Android App</td>\n",
       "      <td>1</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweet      Product  Sentiment  \\\n",
       "4   @sxtxstate great stuff on Fri #SXSW: Marissa M...       Google          1   \n",
       "7   #SXSW is just starting, #CTIA is around the co...      Android          1   \n",
       "10  Excited to meet the @samsungmobileus at #sxsw ...      Android          1   \n",
       "11  Find &amp; Start Impromptu Parties at #SXSW Wi...  Android App          1   \n",
       "12  Foursquare ups the game, just in time for #SXS...  Android App          1   \n",
       "\n",
       "     Brand  \n",
       "4   Google  \n",
       "7   Google  \n",
       "10  Google  \n",
       "11  Google  \n",
       "12  Google  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = apple_df[['Tweet']]\n",
    "# y = apple_df['Sentiment']\n",
    "# X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing 2 Train_Test_Splits on whole Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the train-test split before the Data Preprocessing prevents data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the X DataFrame with only the tweets\n",
    "X = df[['Tweet']]\n",
    "#Create the y Series with only the sentiments, 1 for Positive, 0 for not Positive\n",
    "y = df['Sentiment']\n",
    "#First train test split\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "#Second train test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.601437\n",
       "1    0.398563\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Baseline Understanding\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If we did a multi class\n",
    "# dict_sent = {'No emotion toward brand or product':1, \n",
    "#              'Positive emotion':2,\n",
    "#              'Negative emotion':0,\n",
    "#              \"I can't tell\": 1}\n",
    "# df['Sentiment'] = df['Sentiment'].map(dict_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Preprocess targets\n",
    "# y_train = y_train.apply(lambda x: 1 if x == \"Positive emotion\" else 0)\n",
    "# y_val = y_val.apply(lambda x: 1 if x == \"Positive emotion\" else 0)\n",
    "# y_test = y_test.apply(lambda x: 1 if x == \"Positive emotion\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8339</th>\n",
       "      <td>@mention Takin' the mic at #touchingstories #i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7922</th>\n",
       "      <td>Apple puts up a pop up store for #SXSW folks -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>RT @mention Apple Opening Temporary iPad 2 Sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6178</th>\n",
       "      <td>RT @mention It's not a rumor: Apple is opening...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>#sxsw already paying dividends, discovered a c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet\n",
       "8339  @mention Takin' the mic at #touchingstories #i...\n",
       "7922  Apple puts up a pop up store for #SXSW folks -...\n",
       "5450  RT @mention Apple Opening Temporary iPad 2 Sto...\n",
       "6178  RT @mention It's not a rumor: Apple is opening...\n",
       "2786  #sxsw already paying dividends, discovered a c..."
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3618, 1)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1206, 1)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate necessary tools\n",
    "tokenizer = RegexpTokenizer(r\"(?u)\\w{3,}\")\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "stopwords_list.append(\"sxsw\") #remove sxsw because it's the hashtag for the event\n",
    "lemma = WordNetLemmatizer()\n",
    "tweet_tknzr = TweetTokenizer(strip_handles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'sxsw']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(text):\n",
    "    #Use TweetTokenizer object to remove the handles from the Tweet\n",
    "    #TweetTokenizer also puts each punctuation as it's own token\n",
    "    no_handle = tweet_tknzr.tokenize(text)\n",
    "    #Join the list of non-handle tokens back together\n",
    "    tweet = \" \".join(no_handle) \n",
    "    #remove http websites, hashtag sign, any words in curly brackets,\n",
    "        #any words with ampersand in front, www dot com websites, links,\n",
    "        #videos, and non-english characters\n",
    "    clean = re.sub(\"(https?:\\/\\/\\S+) \\\n",
    "                   |(#[A-Za-z0-9_]+) \\\n",
    "                   |(\\{([a-zA-Z].+)\\}) \\\n",
    "                   |(&[a-z]+;) \\\n",
    "                   |(www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com))\\\n",
    "                   |({link})\\\n",
    "                   |(\\[video\\])\\\n",
    "                   |([^\\x00-\\x7F]+\\ *(?:[^\\x00-\\x7F]| )*)\",\" \", tweet)\n",
    "    #Turn all the tokens lowercase\n",
    "    lower = clean.lower()\n",
    "    #Only include words with 3 or more characters\n",
    "    token_list = tokenizer.tokenize(lower)\n",
    "    #Remove stopwords\n",
    "    stopwords_removed=[token for token in token_list if token not in stopwords_list]\n",
    "    #Lemmatize the remaining word tokens\n",
    "    lemma_list = [lemma.lemmatize(token) for token in stopwords_removed]\n",
    "    #Turn the lemma list into a string for the Vectorizer\n",
    "    cleaned_string = \" \".join(lemma_list) \n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'takin mic touchingstories ipad tablet ipad changing industry rock roll brother'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "clean_tweets(X_train['Tweet'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-173-4c1b95869ad2>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['Tweet'] = X_train['Tweet'].apply(lambda x: clean_tweets(x))\n",
      "<ipython-input-173-4c1b95869ad2>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val['Tweet'] = X_val['Tweet'].apply(lambda x: clean_tweets(x))\n",
      "<ipython-input-173-4c1b95869ad2>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['Tweet'] = X_test['Tweet'].apply(lambda x: clean_tweets(x))\n"
     ]
    }
   ],
   "source": [
    "#Apply our clean_tweets function to X_train, X_val, X_test\n",
    "X_train['Tweet'] = X_train['Tweet'].apply(lambda x: clean_tweets(x))\n",
    "X_val['Tweet'] = X_val['Tweet'].apply(lambda x: clean_tweets(x))\n",
    "X_test['Tweet'] = X_test['Tweet'].apply(lambda x: clean_tweets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8339</th>\n",
       "      <td>takin mic touchingstories ipad tablet ipad cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7922</th>\n",
       "      <td>apple put pop store folk awesome marketing lin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>apple opening temporary ipad store handle dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6178</th>\n",
       "      <td>rumor apple opening temporary store downtown a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>already paying dividend discovered couple cool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6254</th>\n",
       "      <td>line ipad longer today yesterday getting line ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>2011 novelty ipad news apps fade fast among di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8875</th>\n",
       "      <td>offer apple tinfoil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2174</th>\n",
       "      <td>launched spin ipad app ton music photo video w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>one say apple drupal tribe know people care ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3618 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet\n",
       "8339  takin mic touchingstories ipad tablet ipad cha...\n",
       "7922  apple put pop store folk awesome marketing lin...\n",
       "5450  apple opening temporary ipad store handle dema...\n",
       "6178  rumor apple opening temporary store downtown a...\n",
       "2786  already paying dividend discovered couple cool...\n",
       "...                                                 ...\n",
       "6254  line ipad longer today yesterday getting line ...\n",
       "1769  2011 novelty ipad news apps fade fast among di...\n",
       "8875                                offer apple tinfoil\n",
       "2174  launched spin ipad app ton music photo video w...\n",
       "7011  one say apple drupal tribe know people care ch...\n",
       "\n",
       "[3618 rows x 1 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3237</th>\n",
       "      <td>publishing website try mpact app iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4061</th>\n",
       "      <td>josh clark session ipad tablet development fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>apple security guy lock store link lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6444</th>\n",
       "      <td>photo spending sat night chatting husband skyp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>apple elegant fascist company america flipboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>got flask ipad yes sitting 9th waiting aicn sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>flipboard bldg app appstore need wholistic mkt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>think literally everyone iphone guy dropped ip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7267</th>\n",
       "      <td>curious ipad sale went austin lot potential ip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6251</th>\n",
       "      <td>line pop apple store austin first person wait ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1206 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet\n",
       "3237            publishing website try mpact app iphone\n",
       "4061  josh clark session ipad tablet development fir...\n",
       "1320             apple security guy lock store link lol\n",
       "6444  photo spending sat night chatting husband skyp...\n",
       "5168    apple elegant fascist company america flipboard\n",
       "...                                                 ...\n",
       "2117  got flask ipad yes sitting 9th waiting aicn sc...\n",
       "2428  flipboard bldg app appstore need wholistic mkt...\n",
       "3917  think literally everyone iphone guy dropped ip...\n",
       "7267  curious ipad sale went austin lot potential ip...\n",
       "6251  line pop apple store austin first person wait ...\n",
       "\n",
       "[1206 rows x 1 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8339    1\n",
       "7922    1\n",
       "5450    0\n",
       "6178    0\n",
       "2786    1\n",
       "       ..\n",
       "6254    0\n",
       "1769    0\n",
       "8875    0\n",
       "2174    1\n",
       "7011    0\n",
       "Name: Sentiment, Length: 3618, dtype: int64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3237    0\n",
       "4061    1\n",
       "1320    0\n",
       "6444    0\n",
       "5168    0\n",
       "       ..\n",
       "2117    1\n",
       "2428    0\n",
       "3917    0\n",
       "7267    0\n",
       "6251    0\n",
       "Name: Sentiment, Length: 1206, dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DON'T NEED BECAUSE I ADDED A LINE TO THE CLEAN_TWEETS FUNCTION\n",
    "\n",
    "# X_train[\"Tweet\"] = X_train[\"Tweet\"].str.join(\" \")\n",
    "# X_val[\"Tweet\"] = X_val[\"Tweet\"].str.join(\" \")\n",
    "# X_test[\"Tweet\"] = X_test[\"Tweet\"].str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3618x5022 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 33278 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vectorizer = CountVectorizer()\n",
    "c_vectorizer.fit(X_train['Tweet'])\n",
    "X_train_c_vec = c_vectorizer.transform(X_train['Tweet'])\n",
    "X_train_c_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>0310apple</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>106</th>\n",
       "      <th>10k</th>\n",
       "      <th>10mins</th>\n",
       "      <th>10pm</th>\n",
       "      <th>10x</th>\n",
       "      <th>11am</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeus</th>\n",
       "      <th>zimride</th>\n",
       "      <th>zinio</th>\n",
       "      <th>zip</th>\n",
       "      <th>zms</th>\n",
       "      <th>zomg</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zzzs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8339</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7922</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5450</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6178</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6254</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8875</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2174</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3618 rows × 5022 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      000  0310apple  100  101  106  10k  10mins  10pm  10x  11am  ...  zero  \\\n",
       "8339    0          0    0    0    0    0       0     0    0     0  ...     0   \n",
       "7922    0          0    0    0    0    0       0     0    0     0  ...     0   \n",
       "5450    0          0    0    0    0    0       0     0    0     0  ...     0   \n",
       "6178    0          0    0    0    0    0       0     0    0     0  ...     0   \n",
       "2786    0          0    0    0    0    0       0     0    0     0  ...     0   \n",
       "...   ...        ...  ...  ...  ...  ...     ...   ...  ...   ...  ...   ...   \n",
       "6254    0          0    0    0    0    0       0     0    0     0  ...     0   \n",
       "1769    0          0    0    0    0    0       0     0    0     0  ...     0   \n",
       "8875    0          0    0    0    0    0       0     0    0     0  ...     0   \n",
       "2174    0          0    0    0    0    0       0     0    0     0  ...     0   \n",
       "7011    0          0    0    0    0    0       0     0    0     0  ...     0   \n",
       "\n",
       "      zeus  zimride  zinio  zip  zms  zomg  zone  zuckerberg  zzzs  \n",
       "8339     0        0      0    0    0     0     0           0     0  \n",
       "7922     0        0      0    0    0     0     0           0     0  \n",
       "5450     0        0      0    0    0     0     0           0     0  \n",
       "6178     0        0      0    0    0     0     0           0     0  \n",
       "2786     0        0      0    0    0     0     0           0     0  \n",
       "...    ...      ...    ...  ...  ...   ...   ...         ...   ...  \n",
       "6254     0        0      0    0    0     0     0           0     0  \n",
       "1769     0        0      0    0    0     0     0           0     0  \n",
       "8875     0        0      0    0    0     0     0           0     0  \n",
       "2174     0        0      0    0    0     0     0           0     0  \n",
       "7011     0        0      0    0    0     0     0           0     0  \n",
       "\n",
       "[3618 rows x 5022 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_c_vec_df = pd.DataFrame(X_train_c_vec.toarray(), columns=c_vectorizer.get_feature_names(), \n",
    "                              index=X_train.index)\n",
    "X_train_c_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Sanity Check\n",
    "# X_train_c_vec_df['sxsw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_c_vec = c_vectorizer.transform(X_val['Tweet'])\n",
    "X_val_c_vec_df = pd.DataFrame(X_val_c_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3618, 5022)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(X_train['Tweet'])\n",
    "X_train_tfidf_vec = tfidf_vectorizer.transform(X_train['Tweet'])\n",
    "X_val_tfidf_vec = tfidf_vectorizer.transform(X_val['Tweet'])\n",
    "X_train_tfidf_vec_df = pd.DataFrame(X_train_tfidf_vec.toarray())\n",
    "X_val_tfidf_vec_df = pd.DataFrame(X_val_tfidf_vec.toarray())\n",
    "X_train_tfidf_vec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Logistic Regression Model w/ Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.900497512437811\n",
      "0.6990049751243781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train_c_vec_df, y_train)\n",
    "print(lr.score(X_train_c_vec_df, y_train))\n",
    "print(lr.score(X_val_c_vec_df, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Logistic Regression Model w/ Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8228302929795467\n",
      "0.6965174129353234\n"
     ]
    }
   ],
   "source": [
    "lr_2 = LogisticRegression()\n",
    "lr_2.fit(X_train_tfidf_vec_df, y_train)\n",
    "print(lr_2.score(X_train_tfidf_vec_df, y_train))\n",
    "print(lr_2.score(X_val_tfidf_vec_df, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes w/ Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8598673300165838\n",
      "0.6807628524046434\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_c_vec_df, y_train)\n",
    "print(naive_bayes.score(X_train_c_vec_df, y_train))\n",
    "print(naive_bayes.score(X_val_c_vec_df, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes w/ Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8272526257600884\n",
      "0.685737976782753\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_2 = MultinomialNB()\n",
    "naive_bayes_2.fit(X_train_tfidf_vec_df, y_train)\n",
    "print(naive_bayes_2.score(X_train_tfidf_vec_df, y_train))\n",
    "print(naive_bayes_2.score(X_val_tfidf_vec_df, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes w/ Tuned Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vectorizer_2 = CountVectorizer(max_df=.99,min_df=2, max_features=1000)\n",
    "    #max_df=.95,  # removes words that appear in more than 95% of docs\n",
    "    #min_df=2     # removes words that appear 2 or fewer times\n",
    "c_vectorizer_2.fit(X_train['Tweet'])\n",
    "X_train_c_vec_2 = c_vectorizer_2.transform(X_train['Tweet'])\n",
    "X_val_c_vec_2 = c_vectorizer_2.transform(X_val['Tweet'])\n",
    "X_train_c_vec_df_2 = pd.DataFrame(X_train_c_vec_2.toarray())\n",
    "X_val_c_vec_df_2 = pd.DataFrame(X_val_c_vec_2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes with tuned count vectorizer\n",
      "0.7681039248203427\n",
      "0.6832504145936982\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_3 = MultinomialNB()\n",
    "naive_bayes_3.fit(X_train_c_vec_df_2, y_train)\n",
    "print(\"naive bayes with tuned count vectorizer\")\n",
    "print(naive_bayes_3.score(X_train_c_vec_df_2, y_train))\n",
    "print(naive_bayes_3.score(X_val_c_vec_df_2, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHOLE DATASET\n",
    "# naive bayes with tuned count vectorizer\n",
    "# 0.7582094429014867\n",
    "# 0.705046545810877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer_2 = TfidfVectorizer(max_df=.99,min_df=0.005, max_features=1000)\n",
    "tfidf_vectorizer_2.fit(X_train['Tweet'])\n",
    "X_train_tfidf_vec_2 = tfidf_vectorizer_2.transform(X_train['Tweet'])\n",
    "X_val_tfidf_vec_2 = tfidf_vectorizer_2.transform(X_val['Tweet'])\n",
    "X_train_tfidf_vec_df_2 = pd.DataFrame(X_train_tfidf_vec_2.toarray())\n",
    "X_val_tfidf_vec_df_2 = pd.DataFrame(X_val_tfidf_vec_2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes with tuned tfidf\n",
      "0.701216141514649\n",
      "0.675787728026534\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_4 = MultinomialNB()\n",
    "naive_bayes_4.fit(X_train_tfidf_vec_df_2, y_train)\n",
    "print(\"naive bayes with tuned tfidf\")\n",
    "print(naive_bayes_4.score(X_train_tfidf_vec_df_2, y_train))\n",
    "print(naive_bayes_4.score(X_val_tfidf_vec_df_2, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHOLE DATASET\n",
    "# naive bayes with tuned tfidf\n",
    "# 0.7162228394053259\n",
    "# 0.7011268985791279"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with max iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default count vectorizer\n",
      "0.900497512437811\n",
      "0.6990049751243781\n"
     ]
    }
   ],
   "source": [
    "lr_3 = LogisticRegression(max_iter=1000)\n",
    "lr_3.fit(X_train_c_vec_df, y_train)\n",
    "print(\"default count vectorizer\")\n",
    "print(lr_3.score(X_train_c_vec_df, y_train))\n",
    "print(lr_3.score(X_val_c_vec_df, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default tfidf vectorizer\n",
      "0.8228302929795467\n",
      "0.6965174129353234\n"
     ]
    }
   ],
   "source": [
    "#lr_3 = LogisticRegression(max_iter=1000)\n",
    "lr_3.fit(X_train_tfidf_vec_df, y_train)\n",
    "print(\"default tfidf vectorizer\")\n",
    "print(lr_3.score(X_train_tfidf_vec_df, y_train))\n",
    "print(lr_3.score(X_val_tfidf_vec_df, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf vectorizer with tuned parameters\n",
      "0.7252625760088447\n",
      "0.6824212271973465\n"
     ]
    }
   ],
   "source": [
    "#lr_3 = LogisticRegression(max_iter=1000)\n",
    "lr_3.fit(X_train_tfidf_vec_df_2, y_train)\n",
    "print(\"tfidf vectorizer with tuned parameters\")\n",
    "print(lr_3.score(X_train_tfidf_vec_df_2, y_train))\n",
    "print(lr_3.score(X_val_tfidf_vec_df_2, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA w/ Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a StandardScaler Object\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_c_vec_df) #Use default count vectorizer\n",
    "X_val_scaled = scaler.transform(X_val_c_vec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1844"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to import, instantiate and fit a PCA object\n",
    "pca = PCA(n_components = .95, random_state=42)\n",
    "pca.fit(X_train_scaled)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA with n_components=0.95, default count vectorizer, and logistic regression\n",
      "0.9494195688225538\n",
      "0.6799336650082919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# Construct a pipelines\n",
    "pipe_lr = Pipeline([('pca', pca), \n",
    "                    ('lr', LogisticRegression(random_state=42, max_iter=1000))])\n",
    "pipe_lr.fit(X_train_scaled, y_train)\n",
    "print(\"PCA with n_components=0.95, default count vectorizer, and logistic regression\")\n",
    "print(pipe_lr.score(X_train_scaled, y_train))\n",
    "print(pipe_lr.score(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe_mnb = Pipeline([('pca', pca), \n",
    "#                     ('mnb', MultinomialNB())])\n",
    "# pipe_mnb.fit(X_train_scaled, y_train)\n",
    "# print(\"PCA with n_components=0.95, default count vectorizer, and naive bayes\")\n",
    "# print(pipe_lr.score(X_train_scaled, y_train))\n",
    "# print(pipe_lr.score(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I got an error about MultinomialNB not having negative values*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA w/ Tuned TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a StandardScaler Object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Fit scaler to tuned vectorized X_train and transform both X_train and X_val\n",
    "X_train_scaled = scaler.fit_transform(X_train_tfidf_vec_df_2)\n",
    "X_val_scaled = scaler.transform(X_val_tfidf_vec_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a PCA object\n",
    "pca = PCA(n_components = .99, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train_scaled)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pca', PCA(n_components=0.99, random_state=42)),\n",
       "                ('lr', LogisticRegression(max_iter=1000, random_state=42))])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Pipeline object for PCA and Logistic Regression\n",
    "pipe_lr = Pipeline([('pca', pca), \n",
    "                    ('lr', LogisticRegression(random_state=42, max_iter=1000))])\n",
    "pipe_lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA with n_components=0.99, tuned tfidf vectorizer, and naive bayes\n",
      "0.7252625760088447\n",
      "0.681592039800995\n"
     ]
    }
   ],
   "source": [
    "#Check results\n",
    "print(\"PCA with n_components=0.99, tuned tfidf vectorizer, and naive bayes\")\n",
    "print(pipe_lr.score(X_train_scaled, y_train))\n",
    "print(pipe_lr.score(X_val_scaled, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with n_components=0.99, tuned tfidf vectorizer, and naive bayes\n",
    "# 0.7250449272994609\n",
    "# 0.7099461048505634"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline and Cross Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, plot_confusion_matrix, plot_roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garrettwilliams/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/garrettwilliams/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/garrettwilliams/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/garrettwilliams/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.14137435, 0.14024305, 0.138978  , 0.13517618, 0.13740969]),\n",
       " 'score_time': array([0.01971579, 0.01860189, 0.01813698, 0.01784301, 0.01842141]),\n",
       " 'test_accuracy': array([0.74040816, 0.71486928, 0.75571895, 0.71405229, 0.73856209]),\n",
       " 'train_accuracy': array([0.90012255, 0.90095977, 0.8989177 , 0.90300184, 0.90014294]),\n",
       " 'test_precision': array([0.64468864, 0.58545455, 0.68301887, 0.5777027 , 0.64393939]),\n",
       " 'train_precision': array([0.93008641, 0.93035994, 0.92182663, 0.93977813, 0.9335443 ]),\n",
       " 'test_roc_auc': array([0.76039043, 0.72168393, 0.75111928, 0.72181965, 0.72928713]),\n",
       " 'train_roc_auc': array([0.9658115 , 0.96702756, 0.96351533, 0.96518872, 0.96585212])}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_logreg = Pipeline(steps=[\n",
    "    ('count_vectorizer', CountVectorizer()),\n",
    "    ('logreg', LogisticRegression(random_state=42))\n",
    "])\n",
    "cv = cross_validate(pipe_logreg, X_train['Tweet'], y_train, return_train_score=True, \\\n",
    "                    scoring=['accuracy', 'precision','roc_auc'])\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Naive Bayes w/ Default Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.0408442 , 0.03933597, 0.03985906, 0.03992701, 0.04186916]),\n",
       " 'score_time': array([0.01813602, 0.01813412, 0.01827288, 0.01857829, 0.01944375]),\n",
       " 'test_accuracy': array([0.70612245, 0.70506536, 0.69934641, 0.69444444, 0.70915033]),\n",
       " 'train_accuracy': array([0.78696895, 0.7945681 , 0.78742087, 0.79252604, 0.79048397]),\n",
       " 'test_recall': array([0.15365239, 0.14646465, 0.12626263, 0.13636364, 0.16161616]),\n",
       " 'train_recall': array([0.35669192, 0.38170347, 0.35772871, 0.37223975, 0.36719243]),\n",
       " 'test_roc_auc': array([0.73155094, 0.6767448 , 0.69595411, 0.70236178, 0.70092379]),\n",
       " 'train_roc_auc': array([0.88950838, 0.89780161, 0.89179296, 0.89564979, 0.89119786])}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a pipeline to use an Untuned TfidfVectorizer() and MultinomialNB()\n",
    "pipe_nb = Pipeline(steps=[\n",
    "    ('tfidf_vectorizer', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "#Cross validate\n",
    "cv = cross_validate(pipe_nb, X_train['Tweet'], y_train, return_train_score=True, \\\n",
    "                    scoring=['accuracy', 'recall','roc_auc'])\n",
    "#See the results\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7028257969854609"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get mean accuracy for validation set\n",
    "cv['test_accuracy'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Naive Bayes w/ Tuned Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.04241419, 0.04056215, 0.03925681, 0.03921103, 0.039891  ]),\n",
       " 'score_time': array([0.01800704, 0.01839113, 0.01804519, 0.01959586, 0.01807213]),\n",
       " 'test_accuracy': array([0.71265306, 0.7001634 , 0.70588235, 0.68954248, 0.70343137]),\n",
       " 'train_accuracy': array([0.71486928, 0.71880743, 0.71554013, 0.7177864 , 0.71860323]),\n",
       " 'test_recall': array([0.19647355, 0.18181818, 0.16414141, 0.17676768, 0.16666667]),\n",
       " 'train_recall': array([0.19318182, 0.20694006, 0.18927445, 0.21514196, 0.20315457]),\n",
       " 'test_roc_auc': array([0.69983816, 0.65763309, 0.68208352, 0.66534762, 0.6631487 ]),\n",
       " 'train_roc_auc': array([0.72634787, 0.73428142, 0.72852908, 0.73547677, 0.73252431])}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a pipeline to use an Tuned TfidfVectorizer() and MultinomialNB()\n",
    "pipe_nb_tuned = Pipeline(steps=[\n",
    "    ('tfidf_vectorizer_tuned', TfidfVectorizer(max_df=.99,min_df=0.005, max_features=1000)),\n",
    "    ('nb_tuned', MultinomialNB())\n",
    "])\n",
    "#Cross validate\n",
    "cv = cross_validate(pipe_nb_tuned, X_train['Tweet'], y_train, return_train_score=True, \\\n",
    "                    scoring=['accuracy', 'recall','roc_auc'])\n",
    "#See the results\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([1.35563898, 1.32724214, 1.3579073 , 1.32797313, 1.31642294]),\n",
       " 'score_time': array([0.09072995, 0.08811212, 0.09062457, 0.0879178 , 0.08778   ]),\n",
       " 'test_accuracy': array([0.71510204, 0.71568627, 0.71650327, 0.72058824, 0.70915033]),\n",
       " 'train_accuracy': array([0.95118464, 0.94853992, 0.94894834, 0.94813151, 0.94935675]),\n",
       " 'test_precision': array([0.59677419, 0.59022556, 0.59760956, 0.608     , 0.57936508]),\n",
       " 'train_precision': array([0.96030116, 0.95997239, 0.96450939, 0.95676047, 0.96650384]),\n",
       " 'test_roc_auc': array([0.71998169, 0.71112087, 0.70720185, 0.71120474, 0.70237856]),\n",
       " 'train_roc_auc': array([0.99131925, 0.99003833, 0.99008938, 0.98925492, 0.98986803])}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rfc = Pipeline(steps=[\n",
    "    ('tfidf_vectorizer', TfidfVectorizer(max_df=.99,min_df=0.005, max_features=1000)),\n",
    "    ('rfc', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "cv = cross_validate(pipe_rfc, X_train['Tweet'], y_train, return_train_score=True, \\\n",
    "                    scoring=['accuracy', 'precision','roc_auc'])\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('tfidf_vectorizer',\n",
       "                                        TfidfVectorizer(max_df=0.99,\n",
       "                                                        max_features=1000,\n",
       "                                                        min_df=0.005)),\n",
       "                                       ('rfc',\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             param_grid={'rfc__class_weight': ['balanced'],\n",
       "                         'rfc__max_depth': [25, 50, 100],\n",
       "                         'rfc__min_samples_leaf': [1, 3, 5],\n",
       "                         'rfc__n_estimators': [500, 1000, 1500],\n",
       "                         'rfc__random_state': [42]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_rfc = {\n",
    "    \"rfc__max_depth\" :[25, 50, 100],\n",
    "    \"rfc__min_samples_leaf\" : [1, 3, 5],\n",
    "    \"rfc__n_estimators\": [500, 1000, 1500],\n",
    "    \"rfc__class_weight\" :['balanced'],\n",
    "    \"rfc__random_state\":[42]\n",
    "}\n",
    "grid_rfc = GridSearchCV(estimator = pipe_rfc, param_grid=pg_rfc, scoring='accuracy',\n",
    "                        return_train_score = True)\n",
    "grid_rfc.fit(X_train['Tweet'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_rfc__class_weight</th>\n",
       "      <th>param_rfc__max_depth</th>\n",
       "      <th>param_rfc__min_samples_leaf</th>\n",
       "      <th>param_rfc__n_estimators</th>\n",
       "      <th>param_rfc__random_state</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.318658</td>\n",
       "      <td>0.058411</td>\n",
       "      <td>0.104045</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.703673</td>\n",
       "      <td>0.692810</td>\n",
       "      <td>0.695261</td>\n",
       "      <td>0.693627</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.692369</td>\n",
       "      <td>0.008840</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.573491</td>\n",
       "      <td>0.096270</td>\n",
       "      <td>0.190555</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.702041</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>0.695261</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>0.693513</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.840107</td>\n",
       "      <td>0.197343</td>\n",
       "      <td>0.282089</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.701224</td>\n",
       "      <td>0.687092</td>\n",
       "      <td>0.702614</td>\n",
       "      <td>0.695261</td>\n",
       "      <td>0.681373</td>\n",
       "      <td>0.693513</td>\n",
       "      <td>0.008173</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.624615</td>\n",
       "      <td>0.020397</td>\n",
       "      <td>0.093181</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.686531</td>\n",
       "      <td>0.678105</td>\n",
       "      <td>0.681373</td>\n",
       "      <td>0.682190</td>\n",
       "      <td>0.658497</td>\n",
       "      <td>0.677339</td>\n",
       "      <td>0.009797</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.238793</td>\n",
       "      <td>0.054916</td>\n",
       "      <td>0.177897</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.692245</td>\n",
       "      <td>0.673203</td>\n",
       "      <td>0.687092</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>0.662582</td>\n",
       "      <td>0.679789</td>\n",
       "      <td>0.010623</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.739683</td>\n",
       "      <td>0.064339</td>\n",
       "      <td>0.270101</td>\n",
       "      <td>0.023248</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.693061</td>\n",
       "      <td>0.670752</td>\n",
       "      <td>0.687092</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.679789</td>\n",
       "      <td>0.011646</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.456939</td>\n",
       "      <td>0.015529</td>\n",
       "      <td>0.089320</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.679184</td>\n",
       "      <td>0.667484</td>\n",
       "      <td>0.678105</td>\n",
       "      <td>0.678105</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.672928</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.861272</td>\n",
       "      <td>0.034160</td>\n",
       "      <td>0.169892</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.683265</td>\n",
       "      <td>0.665033</td>\n",
       "      <td>0.687092</td>\n",
       "      <td>0.673203</td>\n",
       "      <td>0.660131</td>\n",
       "      <td>0.673745</td>\n",
       "      <td>0.010299</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.256708</td>\n",
       "      <td>0.030177</td>\n",
       "      <td>0.247809</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>balanced</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.684898</td>\n",
       "      <td>0.660948</td>\n",
       "      <td>0.688725</td>\n",
       "      <td>0.674837</td>\n",
       "      <td>0.659314</td>\n",
       "      <td>0.673744</td>\n",
       "      <td>0.012017</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.277533</td>\n",
       "      <td>0.067883</td>\n",
       "      <td>0.135058</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.706939</td>\n",
       "      <td>0.705065</td>\n",
       "      <td>0.710784</td>\n",
       "      <td>0.709967</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>0.706257</td>\n",
       "      <td>0.004379</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.576495</td>\n",
       "      <td>0.056847</td>\n",
       "      <td>0.261771</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.708571</td>\n",
       "      <td>0.704248</td>\n",
       "      <td>0.711601</td>\n",
       "      <td>0.709967</td>\n",
       "      <td>0.702614</td>\n",
       "      <td>0.707401</td>\n",
       "      <td>0.003419</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.737133</td>\n",
       "      <td>0.172062</td>\n",
       "      <td>0.389302</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.706939</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>0.707516</td>\n",
       "      <td>0.706699</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>0.703643</td>\n",
       "      <td>0.004183</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.290345</td>\n",
       "      <td>0.013868</td>\n",
       "      <td>0.115204</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.695510</td>\n",
       "      <td>0.690359</td>\n",
       "      <td>0.685458</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.674837</td>\n",
       "      <td>0.686488</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.540483</td>\n",
       "      <td>0.033923</td>\n",
       "      <td>0.221917</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.694694</td>\n",
       "      <td>0.688725</td>\n",
       "      <td>0.687908</td>\n",
       "      <td>0.687908</td>\n",
       "      <td>0.669935</td>\n",
       "      <td>0.685834</td>\n",
       "      <td>0.008346</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.789670</td>\n",
       "      <td>0.057322</td>\n",
       "      <td>0.340070</td>\n",
       "      <td>0.021676</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.688725</td>\n",
       "      <td>0.684641</td>\n",
       "      <td>0.688725</td>\n",
       "      <td>0.670752</td>\n",
       "      <td>0.685344</td>\n",
       "      <td>0.007863</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.989135</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>0.109460</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.689796</td>\n",
       "      <td>0.674020</td>\n",
       "      <td>0.675654</td>\n",
       "      <td>0.674837</td>\n",
       "      <td>0.657680</td>\n",
       "      <td>0.674397</td>\n",
       "      <td>0.010183</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.948893</td>\n",
       "      <td>0.032455</td>\n",
       "      <td>0.209460</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.689796</td>\n",
       "      <td>0.671569</td>\n",
       "      <td>0.678105</td>\n",
       "      <td>0.677288</td>\n",
       "      <td>0.662582</td>\n",
       "      <td>0.675868</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.881535</td>\n",
       "      <td>0.042614</td>\n",
       "      <td>0.309699</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>balanced</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.692245</td>\n",
       "      <td>0.670752</td>\n",
       "      <td>0.674837</td>\n",
       "      <td>0.675654</td>\n",
       "      <td>0.662582</td>\n",
       "      <td>0.675214</td>\n",
       "      <td>0.009694</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.203749</td>\n",
       "      <td>0.024041</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.697959</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.703431</td>\n",
       "      <td>0.706699</td>\n",
       "      <td>0.703431</td>\n",
       "      <td>0.701520</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12.346109</td>\n",
       "      <td>0.076506</td>\n",
       "      <td>0.324994</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.697959</td>\n",
       "      <td>0.701797</td>\n",
       "      <td>0.700980</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.701797</td>\n",
       "      <td>0.701683</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18.480057</td>\n",
       "      <td>0.111807</td>\n",
       "      <td>0.483685</td>\n",
       "      <td>0.005899</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.698776</td>\n",
       "      <td>0.702614</td>\n",
       "      <td>0.697712</td>\n",
       "      <td>0.706699</td>\n",
       "      <td>0.703431</td>\n",
       "      <td>0.701847</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.815874</td>\n",
       "      <td>0.033027</td>\n",
       "      <td>0.132577</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.688980</td>\n",
       "      <td>0.687908</td>\n",
       "      <td>0.690359</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.677288</td>\n",
       "      <td>0.686162</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.583226</td>\n",
       "      <td>0.061434</td>\n",
       "      <td>0.255994</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.688980</td>\n",
       "      <td>0.682190</td>\n",
       "      <td>0.690359</td>\n",
       "      <td>0.687908</td>\n",
       "      <td>0.678922</td>\n",
       "      <td>0.685672</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.360275</td>\n",
       "      <td>0.109339</td>\n",
       "      <td>0.379568</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.692245</td>\n",
       "      <td>0.683824</td>\n",
       "      <td>0.688725</td>\n",
       "      <td>0.690359</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.686325</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.271639</td>\n",
       "      <td>0.031224</td>\n",
       "      <td>0.119724</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.688163</td>\n",
       "      <td>0.660131</td>\n",
       "      <td>0.674020</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.667484</td>\n",
       "      <td>0.673254</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.529314</td>\n",
       "      <td>0.076020</td>\n",
       "      <td>0.231062</td>\n",
       "      <td>0.003380</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.697143</td>\n",
       "      <td>0.670752</td>\n",
       "      <td>0.674837</td>\n",
       "      <td>0.672386</td>\n",
       "      <td>0.667484</td>\n",
       "      <td>0.676520</td>\n",
       "      <td>0.010585</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.768269</td>\n",
       "      <td>0.110884</td>\n",
       "      <td>0.346861</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>balanced</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>1500</td>\n",
       "      <td>42</td>\n",
       "      <td>{'rfc__class_weight': 'balanced', 'rfc__max_de...</td>\n",
       "      <td>0.694694</td>\n",
       "      <td>0.669118</td>\n",
       "      <td>0.674837</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.676357</td>\n",
       "      <td>0.009848</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        2.318658      0.058411         0.104045        0.008570   \n",
       "1        4.573491      0.096270         0.190555        0.002927   \n",
       "2        6.840107      0.197343         0.282089        0.002942   \n",
       "3        1.624615      0.020397         0.093181        0.001943   \n",
       "4        3.238793      0.054916         0.177897        0.002050   \n",
       "5        4.739683      0.064339         0.270101        0.023248   \n",
       "6        1.456939      0.015529         0.089320        0.000478   \n",
       "7        2.861272      0.034160         0.169892        0.002214   \n",
       "8        4.256708      0.030177         0.247809        0.001791   \n",
       "9        4.277533      0.067883         0.135058        0.000474   \n",
       "10       8.576495      0.056847         0.261771        0.001782   \n",
       "11      12.737133      0.172062         0.389302        0.001426   \n",
       "12       2.290345      0.013868         0.115204        0.000478   \n",
       "13       4.540483      0.033923         0.221917        0.001201   \n",
       "14       6.789670      0.057322         0.340070        0.021676   \n",
       "15       1.989135      0.011709         0.109460        0.001683   \n",
       "16       3.948893      0.032455         0.209460        0.001994   \n",
       "17       5.881535      0.042614         0.309699        0.002136   \n",
       "18       6.203749      0.024041         0.166000        0.001573   \n",
       "19      12.346109      0.076506         0.324994        0.002842   \n",
       "20      18.480057      0.111807         0.483685        0.005899   \n",
       "21       2.815874      0.033027         0.132577        0.001857   \n",
       "22       5.583226      0.061434         0.255994        0.003229   \n",
       "23       8.360275      0.109339         0.379568        0.005834   \n",
       "24       2.271639      0.031224         0.119724        0.001922   \n",
       "25       4.529314      0.076020         0.231062        0.003380   \n",
       "26       6.768269      0.110884         0.346861        0.008413   \n",
       "\n",
       "   param_rfc__class_weight param_rfc__max_depth param_rfc__min_samples_leaf  \\\n",
       "0                 balanced                   25                           1   \n",
       "1                 balanced                   25                           1   \n",
       "2                 balanced                   25                           1   \n",
       "3                 balanced                   25                           3   \n",
       "4                 balanced                   25                           3   \n",
       "5                 balanced                   25                           3   \n",
       "6                 balanced                   25                           5   \n",
       "7                 balanced                   25                           5   \n",
       "8                 balanced                   25                           5   \n",
       "9                 balanced                   50                           1   \n",
       "10                balanced                   50                           1   \n",
       "11                balanced                   50                           1   \n",
       "12                balanced                   50                           3   \n",
       "13                balanced                   50                           3   \n",
       "14                balanced                   50                           3   \n",
       "15                balanced                   50                           5   \n",
       "16                balanced                   50                           5   \n",
       "17                balanced                   50                           5   \n",
       "18                balanced                  100                           1   \n",
       "19                balanced                  100                           1   \n",
       "20                balanced                  100                           1   \n",
       "21                balanced                  100                           3   \n",
       "22                balanced                  100                           3   \n",
       "23                balanced                  100                           3   \n",
       "24                balanced                  100                           5   \n",
       "25                balanced                  100                           5   \n",
       "26                balanced                  100                           5   \n",
       "\n",
       "   param_rfc__n_estimators param_rfc__random_state  \\\n",
       "0                      500                      42   \n",
       "1                     1000                      42   \n",
       "2                     1500                      42   \n",
       "3                      500                      42   \n",
       "4                     1000                      42   \n",
       "5                     1500                      42   \n",
       "6                      500                      42   \n",
       "7                     1000                      42   \n",
       "8                     1500                      42   \n",
       "9                      500                      42   \n",
       "10                    1000                      42   \n",
       "11                    1500                      42   \n",
       "12                     500                      42   \n",
       "13                    1000                      42   \n",
       "14                    1500                      42   \n",
       "15                     500                      42   \n",
       "16                    1000                      42   \n",
       "17                    1500                      42   \n",
       "18                     500                      42   \n",
       "19                    1000                      42   \n",
       "20                    1500                      42   \n",
       "21                     500                      42   \n",
       "22                    1000                      42   \n",
       "23                    1500                      42   \n",
       "24                     500                      42   \n",
       "25                    1000                      42   \n",
       "26                    1500                      42   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.703673   \n",
       "1   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.702041   \n",
       "2   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.701224   \n",
       "3   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.686531   \n",
       "4   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.692245   \n",
       "5   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.693061   \n",
       "6   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.679184   \n",
       "7   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.683265   \n",
       "8   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.684898   \n",
       "9   {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.706939   \n",
       "10  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.708571   \n",
       "11  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.706939   \n",
       "12  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.695510   \n",
       "13  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.694694   \n",
       "14  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.693878   \n",
       "15  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.689796   \n",
       "16  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.689796   \n",
       "17  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.692245   \n",
       "18  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.697959   \n",
       "19  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.697959   \n",
       "20  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.698776   \n",
       "21  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.688980   \n",
       "22  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.688980   \n",
       "23  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.692245   \n",
       "24  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.688163   \n",
       "25  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.697143   \n",
       "26  {'rfc__class_weight': 'balanced', 'rfc__max_de...           0.694694   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.692810           0.695261           0.693627   \n",
       "1            0.691176           0.698529           0.695261   \n",
       "2            0.687092           0.702614           0.695261   \n",
       "3            0.678105           0.681373           0.682190   \n",
       "4            0.673203           0.687092           0.683824   \n",
       "5            0.670752           0.687092           0.686275   \n",
       "6            0.667484           0.678105           0.678105   \n",
       "7            0.665033           0.687092           0.673203   \n",
       "8            0.660948           0.688725           0.674837   \n",
       "9            0.705065           0.710784           0.709967   \n",
       "10           0.704248           0.711601           0.709967   \n",
       "11           0.698529           0.707516           0.706699   \n",
       "12           0.690359           0.685458           0.686275   \n",
       "13           0.688725           0.687908           0.687908   \n",
       "14           0.688725           0.684641           0.688725   \n",
       "15           0.674020           0.675654           0.674837   \n",
       "16           0.671569           0.678105           0.677288   \n",
       "17           0.670752           0.674837           0.675654   \n",
       "18           0.696078           0.703431           0.706699   \n",
       "19           0.701797           0.700980           0.705882   \n",
       "20           0.702614           0.697712           0.706699   \n",
       "21           0.687908           0.690359           0.686275   \n",
       "22           0.682190           0.690359           0.687908   \n",
       "23           0.683824           0.688725           0.690359   \n",
       "24           0.660131           0.674020           0.676471   \n",
       "25           0.670752           0.674837           0.672386   \n",
       "26           0.669118           0.674837           0.676471   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.676471         0.692369        0.008840                9  \n",
       "1            0.680556         0.693513        0.007407                8  \n",
       "2            0.681373         0.693513        0.008173                7  \n",
       "3            0.658497         0.677339        0.009797               18  \n",
       "4            0.662582         0.679789        0.010623               16  \n",
       "5            0.661765         0.679789        0.011646               17  \n",
       "6            0.661765         0.672928        0.007028               27  \n",
       "7            0.660131         0.673745        0.010299               24  \n",
       "8            0.659314         0.673744        0.012017               25  \n",
       "9            0.698529         0.706257        0.004379                2  \n",
       "10           0.702614         0.707401        0.003419                1  \n",
       "11           0.698529         0.703643        0.004183                3  \n",
       "12           0.674837         0.686488        0.006830               10  \n",
       "13           0.669935         0.685834        0.008346               13  \n",
       "14           0.670752         0.685344        0.007863               15  \n",
       "15           0.657680         0.674397        0.010183               23  \n",
       "16           0.662582         0.675868        0.008900               21  \n",
       "17           0.662582         0.675214        0.009694               22  \n",
       "18           0.703431         0.701520        0.003910                6  \n",
       "19           0.701797         0.701683        0.002531                5  \n",
       "20           0.703431         0.701847        0.003261                4  \n",
       "21           0.677288         0.686162        0.004634               12  \n",
       "22           0.678922         0.685672        0.004373               14  \n",
       "23           0.676471         0.686325        0.005665               11  \n",
       "24           0.667484         0.673254        0.009367               26  \n",
       "25           0.667484         0.676520        0.010585               19  \n",
       "26           0.666667         0.676357        0.009848               20  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_rfc.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__class_weight': 'balanced',\n",
       " 'rfc__max_depth': 50,\n",
       " 'rfc__min_samples_leaf': 1,\n",
       " 'rfc__n_estimators': 1000,\n",
       " 'rfc__random_state': 42}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
